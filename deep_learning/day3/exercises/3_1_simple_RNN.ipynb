{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "3_1_simple_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day3/exercises/3_1_simple_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNl2QA_Rnv5"
      },
      "source": [
        "# 準備"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkwjN1jNVAYy"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFXpiH3EVC1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d011619-7c9c-4910-ffcc-15a25bac98a9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ub7RYdeY6pK"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oql7L19rEsWi"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にDNN_codeフォルダを置くことを仮定しています．必要に応じて，パスを変更してください．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ic2JzkvFX59"
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/rabbit_challenge/DNN_code_colab_ver200425')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feXB1SiLP4OL"
      },
      "source": [
        "# simple RNN\n",
        "### バイナリ加算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tzSWNYwxP4OM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e4a12f9-8b3f-4845-f77a-f99f794f48de"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 2\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "#W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "#W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "#W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "\n",
        "# He\n",
        "#W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "#W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "#W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        #z[:,t+1] = functions.relu(u[:,t+1])\n",
        "        #z[:,t+1] = np.tanh(u[:,t+1]) \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.8990793247689373\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "7 + 85 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.2493808535646123\n",
            "Pred:[0 1 0 1 0 0 1 1]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "120 + 46 = 83\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9064200889775069\n",
            "Pred:[0 0 0 0 0 0 1 0]\n",
            "True:[0 0 1 0 1 0 0 0]\n",
            "7 + 33 = 2\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.8121338369538366\n",
            "Pred:[0 0 0 1 0 0 1 1]\n",
            "True:[0 0 0 1 0 0 0 0]\n",
            "8 + 8 = 19\n",
            "------------\n",
            "iters:400\n",
            "Loss:0.9378703825158563\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "125 + 73 = 210\n",
            "------------\n",
            "iters:500\n",
            "Loss:0.9849911700240461\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "56 + 118 = 107\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.2403627647431672\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 1 0 0 0 1 1]\n",
            "119 + 44 = 158\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.1247376592103449\n",
            "Pred:[0 0 0 1 0 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "122 + 14 = 17\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.7136931573957801\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "73 + 76 = 1\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.8084869838497479\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 1 1 0 1 1 0 1]\n",
            "0 + 109 = 1\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.6272085036153583\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "91 + 122 = 133\n",
            "------------\n",
            "iters:1100\n",
            "Loss:0.3190950389756942\n",
            "Pred:[1 1 1 0 1 1 1 1]\n",
            "True:[1 1 1 0 1 0 1 1]\n",
            "123 + 112 = 239\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.3860902443109986\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "105 + 56 = 161\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.5232995622644026\n",
            "Pred:[1 1 1 0 0 1 1 1]\n",
            "True:[1 1 1 1 0 0 1 1]\n",
            "119 + 124 = 231\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.29411867311430423\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "34 + 30 = 64\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.22618371278033222\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "61 + 117 = 178\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.6384366229245616\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "27 + 97 = 254\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.1742616256726501\n",
            "Pred:[1 0 1 1 1 1 0 1]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "66 + 123 = 189\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.2044610296359897\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 1 0 0]\n",
            "17 + 19 = 32\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.2071039423311315\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "124 + 12 = 136\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.19622073389380562\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "67 + 14 = 81\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.1176272603842889\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "90 + 10 = 100\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.10676112328167456\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "109 + 20 = 129\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.07688962229881602\n",
            "Pred:[1 1 0 0 1 1 1 0]\n",
            "True:[1 1 0 0 1 1 1 0]\n",
            "103 + 103 = 206\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.0652518919762273\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "43 + 95 = 138\n",
            "------------\n",
            "iters:2500\n",
            "Loss:1.2616725449538722\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "59 + 25 = 38\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.44434130559346213\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "113 + 69 = 190\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.36582623192859404\n",
            "Pred:[0 0 0 1 0 0 0 1]\n",
            "True:[0 0 1 1 0 0 0 1]\n",
            "48 + 1 = 17\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.36711634128827064\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "36 + 90 = 126\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.2611781637083297\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "63 + 70 = 133\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.07461300952376472\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "110 + 40 = 150\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.04800692999508422\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "3 + 51 = 54\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.02803131256762517\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "56 + 35 = 91\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.03737437082651667\n",
            "Pred:[1 1 0 0 0 0 1 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "95 + 99 = 194\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.019614275279836106\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "4 + 59 = 63\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.013840353417881254\n",
            "Pred:[1 1 0 0 0 0 0 1]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "84 + 109 = 193\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.010415319127544238\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "36 + 93 = 129\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.018927709336523605\n",
            "Pred:[1 0 1 1 1 1 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "107 + 81 = 188\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.019171800414917742\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "91 + 92 = 183\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.01841649250683543\n",
            "Pred:[0 0 1 1 1 1 0 0]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "7 + 53 = 60\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.006519083159865976\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "116 + 69 = 185\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.004163507026668845\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "16 + 106 = 122\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.005385305802659151\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "60 + 73 = 133\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.012507745120401611\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 1 1 0 0 0 1 1]\n",
            "91 + 8 = 99\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.004890246725103625\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 1 0 1 1 1 0 0]\n",
            "97 + 123 = 220\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.0025671479212074\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "36 + 6 = 42\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.0037525525225030544\n",
            "Pred:[1 1 1 0 1 1 1 0]\n",
            "True:[1 1 1 0 1 1 1 0]\n",
            "112 + 126 = 238\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.003969355693912681\n",
            "Pred:[1 0 1 0 1 0 1 1]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "86 + 85 = 171\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.002185055568032822\n",
            "Pred:[0 1 0 0 1 1 0 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "76 + 0 = 76\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.003408113393284874\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "28 + 43 = 71\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.002457423961628035\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 0 1 0 0 0 1 0]\n",
            "52 + 110 = 162\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.0038445927195688255\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "105 + 82 = 187\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.007450123079680169\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "83 + 40 = 123\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.0014030276977946255\n",
            "Pred:[0 0 1 1 0 0 1 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "48 + 2 = 50\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.002213507969691376\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "42 + 69 = 111\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.010689579722084184\n",
            "Pred:[1 0 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "103 + 34 = 137\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.0019299478385261437\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "125 + 83 = 208\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.008346239638666084\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "83 + 98 = 181\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.001391085598041205\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "120 + 96 = 216\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.0015293047731469488\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "46 + 75 = 121\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.004856128565250996\n",
            "Pred:[0 1 0 0 0 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "43 + 27 = 70\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.00223528028948115\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "65 + 56 = 121\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.001445408217328741\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "2 + 94 = 96\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.003107606730726099\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "75 + 15 = 90\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.0011398343492352184\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "6 + 58 = 64\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.0012370375345197413\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "107 + 109 = 216\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.0016061757499090776\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "82 + 61 = 143\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.001647640059954764\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "13 + 74 = 87\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.0008086829375503202\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "46 + 64 = 110\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.0014192100625316265\n",
            "Pred:[1 0 1 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 0 0]\n",
            "88 + 92 = 180\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.0014869118059162825\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "32 + 43 = 75\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.0009373422460834725\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "34 + 101 = 135\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.0017915713487283214\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "11 + 107 = 118\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.0009519963339046949\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "74 + 92 = 166\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.0008147899549612395\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "3 + 125 = 128\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.0012912813735799823\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "20 + 90 = 110\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.0007269508413270207\n",
            "Pred:[1 0 0 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "18 + 125 = 143\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.0014292642326271918\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "72 + 83 = 155\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.0007574929893631462\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "67 + 53 = 120\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.0008142408928318545\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "34 + 69 = 103\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0008960387664467401\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "15 + 101 = 116\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0006330141302709714\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "75 + 17 = 92\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0007293595516211499\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "112 + 13 = 125\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.0006170994266072359\n",
            "Pred:[0 0 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "54 + 5 = 59\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.0006331473063795274\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "34 + 69 = 103\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.0008433766802643424\n",
            "Pred:[1 0 1 0 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "125 + 47 = 172\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.0007592455272619382\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "112 + 29 = 141\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0030610309972487156\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "123 + 98 = 221\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.0009059769615848602\n",
            "Pred:[1 0 1 0 0 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "101 + 60 = 161\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.00043810213927917636\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "58 + 65 = 123\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.0007410391657438274\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "95 + 37 = 132\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.0005627092624372865\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "87 + 73 = 160\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.0011548784503711458\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "3 + 100 = 103\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0007811473440148425\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "76 + 79 = 155\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.0005323395727726089\n",
            "Pred:[0 0 0 0 1 0 0 1]\n",
            "True:[0 0 0 0 1 0 0 1]\n",
            "0 + 9 = 9\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0005489771505634191\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "69 + 97 = 166\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.0005105519701997546\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "41 + 101 = 142\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0002379113629230667\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "49 + 51 = 100\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0005614250147964255\n",
            "Pred:[0 1 0 1 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "62 + 31 = 93\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.00023258931982423527\n",
            "Pred:[1 0 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 1 0 0]\n",
            "53 + 103 = 156\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7zQEPrtP4OP"
      },
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "## [try] weight_init_stdやlearning_rate, hidden_layer_sizeを変更してみよう\n",
        "\n",
        "\n",
        "## [try] 重みの初期化方法を変更してみよう\n",
        "Xavier, He\n",
        "\n",
        "## [try] 中間層の活性化関数を変更してみよう\n",
        "ReLU(勾配爆発を確認しよう)<br>\n",
        "tanh(numpyにtanhが用意されている。導関数をd_tanhとして作成しよう)\n",
        "\n",
        "---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H-iW4tf0yoIM",
        "outputId": "b5e3a226-45c1-4f4d-f044-37e4c63439a9"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 2\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "#W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "#W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "#W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "\n",
        "# He\n",
        "#W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "#W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "#W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        #z[:,t+1] = functions.relu(u[:,t+1])\n",
        "        #z[:,t+1] = np.tanh(u[:,t+1]) \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.0261798423431616\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "95 + 6 = 0\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0304170992012145\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "123 + 43 = 251\n",
            "------------\n",
            "iters:200\n",
            "Loss:0.9773668714889887\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "1 + 99 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9927961642607963\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "77 + 30 = 255\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.033356489252251\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 0]\n",
            "120 + 22 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0035421674587608\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "11 + 113 = 0\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.1394358668796856\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "10 + 28 = 255\n",
            "------------\n",
            "iters:700\n",
            "Loss:0.8482924420090276\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "96 + 30 = 255\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.9331377891045649\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "64 + 0 = 128\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0108344392566653\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "68 + 18 = 254\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0711802385351819\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "43 + 91 = 255\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0094412409648839\n",
            "Pred:[1 1 1 1 0 0 0 0]\n",
            "True:[1 1 1 0 1 0 1 1]\n",
            "121 + 114 = 240\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.033956177766497\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 0 1]\n",
            "127 + 98 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.9864351359787042\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "39 + 40 = 72\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.9826501437396089\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "96 + 57 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.9173653411221054\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[0 0 1 1 1 0 1 0]\n",
            "5 + 53 = 251\n",
            "------------\n",
            "iters:1600\n",
            "Loss:0.8983533023022794\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "99 + 88 = 255\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.9607237488314014\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "48 + 24 = 96\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.9262898432713452\n",
            "Pred:[1 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "4 + 100 = 236\n",
            "------------\n",
            "iters:1900\n",
            "Loss:1.0082123637359488\n",
            "Pred:[0 0 0 1 0 1 1 0]\n",
            "True:[0 1 0 0 1 1 0 0]\n",
            "11 + 65 = 22\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.9058316211719452\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "80 + 47 = 255\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.8948358483040596\n",
            "Pred:[0 0 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "60 + 64 = 56\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.0794122837867288\n",
            "Pred:[1 1 1 1 1 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "24 + 104 = 248\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.7366382257082181\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "3 + 123 = 255\n",
            "------------\n",
            "iters:2400\n",
            "Loss:1.1307219203852215\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 1 1 1 0]\n",
            "113 + 125 = 0\n",
            "------------\n",
            "iters:2500\n",
            "Loss:1.0084185360022675\n",
            "Pred:[1 0 1 1 1 1 0 0]\n",
            "True:[1 1 0 0 1 1 0 1]\n",
            "94 + 111 = 188\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.2364080332937313\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "2 + 95 = 255\n",
            "------------\n",
            "iters:2700\n",
            "Loss:1.047464848363358\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "63 + 37 = 126\n",
            "------------\n",
            "iters:2800\n",
            "Loss:0.7921209382178901\n",
            "Pred:[0 1 1 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 0]\n",
            "48 + 56 = 96\n",
            "------------\n",
            "iters:2900\n",
            "Loss:1.0199461450519918\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "13 + 100 = 255\n",
            "------------\n",
            "iters:3000\n",
            "Loss:1.0499675582180967\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 0 0 1 1]\n",
            "62 + 117 = 255\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.9228814309016639\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "59 + 10 = 116\n",
            "------------\n",
            "iters:3200\n",
            "Loss:1.1364448948380441\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "118 + 76 = 252\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.8615288962864541\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "78 + 40 = 92\n",
            "------------\n",
            "iters:3400\n",
            "Loss:1.152387375996686\n",
            "Pred:[1 1 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "111 + 33 = 222\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.917236836623643\n",
            "Pred:[1 1 1 1 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "120 + 32 = 240\n",
            "------------\n",
            "iters:3600\n",
            "Loss:1.0922657888734884\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "94 + 119 = 173\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.6575941609377993\n",
            "Pred:[1 0 1 1 0 0 0 1]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "89 + 88 = 177\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.7596395305354534\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "11 + 115 = 119\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.9439568490926328\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "50 + 70 = 100\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.7605414668070978\n",
            "Pred:[0 0 0 1 0 0 1 1]\n",
            "True:[0 0 1 0 0 0 1 1]\n",
            "27 + 8 = 19\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.8512323156681104\n",
            "Pred:[0 0 0 1 0 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "19 + 72 = 19\n",
            "------------\n",
            "iters:4200\n",
            "Loss:1.0567257649454334\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "123 + 16 = 32\n",
            "------------\n",
            "iters:4300\n",
            "Loss:1.378158732554453\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "57 + 15 = 54\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.6327115673527856\n",
            "Pred:[0 0 1 0 1 1 1 1]\n",
            "True:[0 0 1 0 0 1 1 1]\n",
            "34 + 5 = 47\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.6758975361475259\n",
            "Pred:[0 0 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "88 + 1 = 17\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.7075146891402925\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "61 + 98 = 223\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.8880423756898792\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "90 + 6 = 92\n",
            "------------\n",
            "iters:4800\n",
            "Loss:1.2314436244232954\n",
            "Pred:[1 0 1 1 1 1 1 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "71 + 93 = 190\n",
            "------------\n",
            "iters:4900\n",
            "Loss:1.085933181008981\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "106 + 24 = 114\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.7213586785978416\n",
            "Pred:[1 1 0 1 1 1 0 1]\n",
            "True:[1 0 0 1 1 1 0 1]\n",
            "33 + 124 = 221\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.8352410868648453\n",
            "Pred:[1 1 1 0 1 1 1 1]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "52 + 91 = 239\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.6511192458733236\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "86 + 14 = 108\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.7433513361899908\n",
            "Pred:[1 1 1 1 0 0 0 1]\n",
            "True:[1 1 1 1 1 0 0 1]\n",
            "127 + 122 = 241\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.6564471037777204\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "46 + 52 = 82\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.47561443646487955\n",
            "Pred:[0 0 0 1 1 1 1 0]\n",
            "True:[0 0 0 1 1 1 1 0]\n",
            "30 + 0 = 30\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.9947125633745872\n",
            "Pred:[0 1 1 0 1 1 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "50 + 86 = 108\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.754720636720522\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "47 + 87 = 214\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.4714647640334234\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 1 0 1 1 0 1 1]\n",
            "126 + 93 = 219\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.305188141189364\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "68 + 52 = 120\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.36489302254344147\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "111 + 33 = 144\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.07176829708336423\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "72 + 18 = 90\n",
            "------------\n",
            "iters:6200\n",
            "Loss:1.5976985431652304\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "126 + 1 = 129\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.11778686985255091\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "100 + 116 = 216\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.06075298174215485\n",
            "Pred:[1 0 1 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 1 0 1]\n",
            "85 + 88 = 173\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.3823214234232555\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "13 + 58 = 103\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.07895046120382623\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "96 + 28 = 124\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.2873086305535415\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "4 + 83 = 95\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.07809451417902455\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "100 + 34 = 134\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.127342835747726\n",
            "Pred:[0 1 0 0 1 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "18 + 54 = 72\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.09457023230772492\n",
            "Pred:[1 1 0 1 0 1 1 0]\n",
            "True:[1 1 0 1 0 1 1 0]\n",
            "94 + 120 = 214\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.06299987466681217\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "11 + 43 = 54\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.11428218896978297\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "85 + 91 = 176\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.0697006777830126\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "27 + 122 = 149\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.07574193421935349\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "70 + 108 = 178\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.038597828498774696\n",
            "Pred:[0 1 0 1 1 0 1 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "1 + 90 = 91\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.023907350947828615\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "79 + 102 = 181\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.13031392930363905\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "7 + 121 = 128\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.04073596613309935\n",
            "Pred:[0 1 0 0 1 0 1 0]\n",
            "True:[0 1 0 0 1 0 1 0]\n",
            "47 + 27 = 74\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.02132302126543903\n",
            "Pred:[1 0 0 1 1 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "38 + 114 = 152\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.03448400792777582\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "59 + 6 = 65\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.054811407553395995\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "60 + 27 = 87\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.03210507458921332\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "109 + 55 = 164\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.02822553346376811\n",
            "Pred:[0 0 1 1 0 0 1 1]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "48 + 3 = 51\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.05950095206331761\n",
            "Pred:[1 0 1 1 0 1 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "114 + 69 = 183\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.01657416715427849\n",
            "Pred:[0 1 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 1]\n",
            "55 + 10 = 65\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.03507918629237397\n",
            "Pred:[0 1 1 1 0 0 1 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "14 + 101 = 115\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.027412267686597293\n",
            "Pred:[1 1 0 1 0 0 1 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "86 + 125 = 211\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.012759679903259186\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "85 + 28 = 113\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0165617842760646\n",
            "Pred:[0 1 1 1 1 0 0 0]\n",
            "True:[0 1 1 1 1 0 0 0]\n",
            "43 + 77 = 120\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.01066994212152557\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "34 + 82 = 116\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.012353356214558013\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 1 1 1]\n",
            "18 + 45 = 63\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.015279774946942902\n",
            "Pred:[0 0 0 0 1 0 1 1]\n",
            "True:[0 0 0 0 1 0 1 1]\n",
            "10 + 1 = 11\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.010386406233731906\n",
            "Pred:[0 1 0 0 0 0 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "31 + 36 = 67\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.01240610742718159\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "84 + 43 = 127\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.004775911109602714\n",
            "Pred:[0 1 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "53 + 45 = 98\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.00552630848419448\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "112 + 37 = 149\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.009204451996265515\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "99 + 60 = 159\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.005684075235602571\n",
            "Pred:[0 1 0 1 1 1 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "48 + 44 = 92\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.006491463610555298\n",
            "Pred:[1 0 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "127 + 55 = 182\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dd5FD0ICxitb",
        "outputId": "8b5edb1f-7af2-4035-8fed-eaa024e14066"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 2\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "#W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "#W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "#W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "#W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "#W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "#W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "\n",
        "# He\n",
        "W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        #z[:,t+1] = functions.relu(u[:,t+1])\n",
        "        #z[:,t+1] = np.tanh(u[:,t+1]) \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:0.7694608178034485\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "34 + 61 = 255\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9935160192280147\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 1]\n",
            "70 + 107 = 0\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.1602478809994377\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "125 + 68 = 255\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0323251191923999\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "87 + 44 = 255\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0187563532584447\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 0 1 1 1 1 0 0]\n",
            "16 + 44 = 121\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0217929565694872\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "52 + 89 = 251\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0775640331462035\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 1]\n",
            "124 + 43 = 0\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.0137276213578688\n",
            "Pred:[0 0 0 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "106 + 15 = 29\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.8701666354693552\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "64 + 0 = 1\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.1404489471929953\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[0 1 0 1 0 0 1 0]\n",
            "34 + 48 = 255\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.3679593692890195\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "59 + 100 = 0\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0582145043005453\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 1]\n",
            "24 + 33 = 0\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.0049315571268518\n",
            "Pred:[1 1 1 1 1 1 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "122 + 95 = 253\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.8460836365720585\n",
            "Pred:[1 1 0 1 1 1 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "98 + 101 = 223\n",
            "------------\n",
            "iters:1400\n",
            "Loss:1.0493556134556334\n",
            "Pred:[0 0 0 1 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "79 + 29 = 16\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.0205487448394024\n",
            "Pred:[0 1 1 0 0 0 0 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "23 + 112 = 97\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.010070901361516\n",
            "Pred:[0 0 0 1 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "42 + 111 = 16\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.9404377156170423\n",
            "Pred:[0 0 1 0 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "35 + 80 = 34\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.9947020055220402\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "25 + 52 = 0\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.883203285388203\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "39 + 48 = 71\n",
            "------------\n",
            "iters:2000\n",
            "Loss:1.1623643333962317\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "123 + 22 = 127\n",
            "------------\n",
            "iters:2100\n",
            "Loss:1.0282344471274099\n",
            "Pred:[0 0 0 0 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "52 + 66 = 4\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.8942779881774499\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 0 1 1 0 1 1]\n",
            "3 + 24 = 63\n",
            "------------\n",
            "iters:2300\n",
            "Loss:1.0630587079356255\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "50 + 77 = 32\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.9267235219342802\n",
            "Pred:[0 0 1 0 1 1 1 1]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "19 + 22 = 47\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.9943247863129754\n",
            "Pred:[1 0 0 1 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "72 + 125 = 144\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.0678880961846013\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[1 1 0 0 1 1 0 0]\n",
            "89 + 115 = 162\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.658269888912609\n",
            "Pred:[0 1 0 1 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "44 + 40 = 80\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.18891466848192\n",
            "Pred:[1 1 1 0 1 1 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "23 + 122 = 237\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.8250911513716405\n",
            "Pred:[1 1 0 1 0 1 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "106 + 111 = 213\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.59825564406132\n",
            "Pred:[1 1 0 0 0 0 1 0]\n",
            "True:[1 1 0 1 0 0 1 0]\n",
            "113 + 97 = 194\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.9262975531528485\n",
            "Pred:[1 1 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "58 + 127 = 245\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.8542735205888382\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "40 + 124 = 212\n",
            "------------\n",
            "iters:3300\n",
            "Loss:0.6569999971996463\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "70 + 112 = 148\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.44251512756796335\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 0 0 0 1 0]\n",
            "8 + 26 = 32\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.6516022213093448\n",
            "Pred:[0 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "116 + 3 = 119\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.7974580358248115\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "59 + 46 = 121\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.5255295223167428\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "46 + 57 = 71\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.5542939841411552\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "50 + 102 = 220\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.26859247235990424\n",
            "Pred:[1 0 0 0 1 0 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "32 + 106 = 138\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.2825053657900359\n",
            "Pred:[0 1 0 1 0 1 0 1]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "52 + 33 = 85\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.5088398028992906\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "103 + 11 = 118\n",
            "------------\n",
            "iters:4200\n",
            "Loss:0.250727559938787\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "3 + 72 = 75\n",
            "------------\n",
            "iters:4300\n",
            "Loss:0.43663078141505524\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "89 + 59 = 212\n",
            "------------\n",
            "iters:4400\n",
            "Loss:0.23083545993414756\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "20 + 55 = 75\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.576446943489568\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "120 + 12 = 100\n",
            "------------\n",
            "iters:4600\n",
            "Loss:0.18938320364600772\n",
            "Pred:[0 1 1 1 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 0 1]\n",
            "113 + 8 = 121\n",
            "------------\n",
            "iters:4700\n",
            "Loss:0.23878937242408002\n",
            "Pred:[1 0 1 0 0 1 1 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "114 + 52 = 166\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.3581701297116726\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "20 + 28 = 32\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.4207539937520421\n",
            "Pred:[0 1 0 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 0 1 1]\n",
            "1 + 66 = 71\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.300500336387459\n",
            "Pred:[1 0 0 1 0 1 1 0]\n",
            "True:[1 0 0 1 0 1 1 0]\n",
            "95 + 55 = 150\n",
            "------------\n",
            "iters:5100\n",
            "Loss:0.03567768054847546\n",
            "Pred:[0 1 1 0 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "4 + 97 = 101\n",
            "------------\n",
            "iters:5200\n",
            "Loss:0.17596225221689565\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "52 + 27 = 79\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.10236524243309482\n",
            "Pred:[0 1 1 1 0 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "85 + 31 = 116\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.4401343796768079\n",
            "Pred:[1 1 0 0 0 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "109 + 22 = 195\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.1534373030062974\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "45 + 43 = 88\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.13794721165018392\n",
            "Pred:[0 1 1 1 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "28 + 84 = 112\n",
            "------------\n",
            "iters:5700\n",
            "Loss:0.014408487965955619\n",
            "Pred:[0 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "81 + 33 = 114\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.0890668007689438\n",
            "Pred:[1 0 1 1 1 0 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "61 + 124 = 185\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.031028456456789852\n",
            "Pred:[1 0 1 1 1 0 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "113 + 71 = 184\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.023260740488638378\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "77 + 49 = 126\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.013952940219739023\n",
            "Pred:[1 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "64 + 64 = 128\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.1150400749208396\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "39 + 97 = 136\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.022229381347339604\n",
            "Pred:[0 1 1 0 1 0 1 1]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "6 + 101 = 107\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.010366490393682402\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "36 + 81 = 117\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.02241356305488318\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "30 + 24 = 54\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.04871440013834636\n",
            "Pred:[1 0 1 1 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "121 + 55 = 176\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.022150171052433186\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "70 + 66 = 136\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.00792266079519726\n",
            "Pred:[1 0 1 1 1 1 0 0]\n",
            "True:[1 0 1 1 1 1 0 0]\n",
            "81 + 107 = 188\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.021567454646702906\n",
            "Pred:[1 0 1 1 0 1 0 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "91 + 90 = 181\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.01451773087733203\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "98 + 32 = 130\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.048187397892937425\n",
            "Pred:[1 0 0 0 1 1 0 1]\n",
            "True:[1 0 0 0 1 1 0 1]\n",
            "111 + 30 = 141\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.007791751941640257\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "36 + 75 = 111\n",
            "------------\n",
            "iters:7300\n",
            "Loss:0.022063583292956736\n",
            "Pred:[1 0 0 1 1 1 1 1]\n",
            "True:[1 0 0 1 1 1 1 1]\n",
            "118 + 41 = 159\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.026446530162034312\n",
            "Pred:[1 1 1 0 1 0 0 0]\n",
            "True:[1 1 1 0 1 0 0 0]\n",
            "119 + 113 = 232\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.016405671811486788\n",
            "Pred:[1 0 1 1 0 0 1 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "126 + 52 = 178\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.017849019352268486\n",
            "Pred:[1 1 0 0 1 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "124 + 76 = 200\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.007252636783894441\n",
            "Pred:[1 0 0 1 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "34 + 114 = 148\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.009610861612146127\n",
            "Pred:[1 1 0 1 1 0 1 1]\n",
            "True:[1 1 0 1 1 0 1 1]\n",
            "121 + 98 = 219\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.01130023972653204\n",
            "Pred:[0 1 0 1 0 1 1 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "54 + 33 = 87\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.0156297416072319\n",
            "Pred:[1 1 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 0]\n",
            "98 + 94 = 192\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.0018249099857965306\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 0 0 0]\n",
            "9 + 79 = 88\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.0022877914814167324\n",
            "Pred:[0 1 1 1 1 0 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "19 + 103 = 122\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.012941841246243669\n",
            "Pred:[1 1 0 0 0 0 0 1]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "109 + 84 = 193\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.002200729280217633\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 1 1 0 1]\n",
            "72 + 53 = 125\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.004904531834867854\n",
            "Pred:[1 1 1 1 1 1 0 1]\n",
            "True:[1 1 1 1 1 1 0 1]\n",
            "126 + 127 = 253\n",
            "------------\n",
            "iters:8600\n",
            "Loss:0.004887565448382929\n",
            "Pred:[0 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "5 + 118 = 123\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.0016585960968412653\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "25 + 29 = 54\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.001626049070620211\n",
            "Pred:[1 0 1 1 0 1 1 0]\n",
            "True:[1 0 1 1 0 1 1 0]\n",
            "93 + 89 = 182\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.0029033627221967256\n",
            "Pred:[1 1 1 0 0 1 0 1]\n",
            "True:[1 1 1 0 0 1 0 1]\n",
            "108 + 121 = 229\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.003347207225577204\n",
            "Pred:[0 1 0 1 1 1 0 1]\n",
            "True:[0 1 0 1 1 1 0 1]\n",
            "10 + 83 = 93\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.002591132106687519\n",
            "Pred:[0 0 1 0 1 0 0 1]\n",
            "True:[0 0 1 0 1 0 0 1]\n",
            "38 + 3 = 41\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.00886083632446672\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 1 1 0]\n",
            "24 + 14 = 38\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.0011876823372400128\n",
            "Pred:[1 1 1 0 1 0 1 0]\n",
            "True:[1 1 1 0 1 0 1 0]\n",
            "117 + 117 = 234\n",
            "------------\n",
            "iters:9400\n",
            "Loss:0.003021403195374369\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[0 0 0 0 1 0 1 0]\n",
            "2 + 8 = 10\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.0016181142980621838\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "32 + 79 = 111\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.004739796953410034\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "48 + 62 = 110\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.0037432089013172552\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "110 + 88 = 198\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.0025303373942349573\n",
            "Pred:[1 1 0 1 0 0 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "121 + 87 = 208\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.005705683437002055\n",
            "Pred:[1 1 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 0 1 1 0]\n",
            "127 + 71 = 198\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wMHbJAJw0KCI",
        "outputId": "aeb88d28-86f3-4044-f618-dcd8672f172d"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 2\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "#W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "#W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "#W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "\n",
        "# He\n",
        "#W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "#W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "#W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        #z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        z[:,t+1] = functions.relu(u[:,t+1])\n",
        "        #z[:,t+1] = np.tanh(u[:,t+1]) \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.4495565064216198\n",
            "Pred:[1 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "50 + 69 = 253\n",
            "------------\n",
            "iters:100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 1 1]\n",
            "17 + 126 = 0\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/rabbit_challenge/DNN_code_colab_ver200425/common/functions.py:6: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1 + np.exp(-x))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "iters:200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "64 + 21 = 0\n",
            "------------\n",
            "iters:300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "71 + 69 = 0\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "29 + 78 = 0\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 1 1 0]\n",
            "77 + 1 = 0\n",
            "------------\n",
            "iters:600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "109 + 66 = 0\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 1 1]\n",
            "76 + 71 = 0\n",
            "------------\n",
            "iters:800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "95 + 80 = 0\n",
            "------------\n",
            "iters:900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "23 + 111 = 0\n",
            "------------\n",
            "iters:1000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "50 + 95 = 0\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 0]\n",
            "120 + 54 = 0\n",
            "------------\n",
            "iters:1200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "73 + 124 = 0\n",
            "------------\n",
            "iters:1300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "85 + 30 = 0\n",
            "------------\n",
            "iters:1400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "85 + 1 = 0\n",
            "------------\n",
            "iters:1500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 1 1]\n",
            "10 + 41 = 0\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 0 0 0]\n",
            "3 + 61 = 0\n",
            "------------\n",
            "iters:1700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 1 1 1 0]\n",
            "30 + 16 = 0\n",
            "------------\n",
            "iters:1800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 1 0 1]\n",
            "103 + 94 = 0\n",
            "------------\n",
            "iters:1900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 1 0 0]\n",
            "93 + 111 = 0\n",
            "------------\n",
            "iters:2000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "33 + 40 = 0\n",
            "------------\n",
            "iters:2100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "72 + 100 = 0\n",
            "------------\n",
            "iters:2200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "55 + 1 = 0\n",
            "------------\n",
            "iters:2300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 1 0]\n",
            "64 + 50 = 0\n",
            "------------\n",
            "iters:2400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 1 1 1 1]\n",
            "126 + 113 = 0\n",
            "------------\n",
            "iters:2500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 1 0 0]\n",
            "114 + 114 = 0\n",
            "------------\n",
            "iters:2600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "127 + 9 = 0\n",
            "------------\n",
            "iters:2700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "38 + 117 = 0\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "105 + 40 = 0\n",
            "------------\n",
            "iters:2900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "67 + 120 = 0\n",
            "------------\n",
            "iters:3000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 0]\n",
            "10 + 124 = 0\n",
            "------------\n",
            "iters:3100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "82 + 48 = 0\n",
            "------------\n",
            "iters:3200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 1 0 0 0 0]\n",
            "23 + 25 = 0\n",
            "------------\n",
            "iters:3300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 1 0]\n",
            "79 + 115 = 0\n",
            "------------\n",
            "iters:3400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "24 + 72 = 0\n",
            "------------\n",
            "iters:3500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 1 0 0]\n",
            "120 + 92 = 0\n",
            "------------\n",
            "iters:3600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "127 + 5 = 0\n",
            "------------\n",
            "iters:3700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 0 0 0 0]\n",
            "96 + 48 = 0\n",
            "------------\n",
            "iters:3800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 0 1]\n",
            "13 + 92 = 0\n",
            "------------\n",
            "iters:3900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 1 0 0 0 1]\n",
            "116 + 125 = 0\n",
            "------------\n",
            "iters:4000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 0]\n",
            "52 + 78 = 0\n",
            "------------\n",
            "iters:4100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 0 0]\n",
            "123 + 101 = 0\n",
            "------------\n",
            "iters:4200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "108 + 70 = 0\n",
            "------------\n",
            "iters:4300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "13 + 115 = 0\n",
            "------------\n",
            "iters:4400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "105 + 108 = 0\n",
            "------------\n",
            "iters:4500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "60 + 127 = 0\n",
            "------------\n",
            "iters:4600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 0]\n",
            "41 + 91 = 0\n",
            "------------\n",
            "iters:4700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "88 + 48 = 0\n",
            "------------\n",
            "iters:4800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 0 1 1]\n",
            "16 + 91 = 0\n",
            "------------\n",
            "iters:4900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "71 + 14 = 0\n",
            "------------\n",
            "iters:5000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 1 0]\n",
            "123 + 55 = 0\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "53 + 44 = 0\n",
            "------------\n",
            "iters:5200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "25 + 22 = 0\n",
            "------------\n",
            "iters:5300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 1 1]\n",
            "123 + 104 = 0\n",
            "------------\n",
            "iters:5400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 0 1]\n",
            "97 + 36 = 0\n",
            "------------\n",
            "iters:5500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 0 0]\n",
            "25 + 59 = 0\n",
            "------------\n",
            "iters:5600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 0 0]\n",
            "123 + 93 = 0\n",
            "------------\n",
            "iters:5700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 1 0]\n",
            "60 + 26 = 0\n",
            "------------\n",
            "iters:5800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "73 + 18 = 0\n",
            "------------\n",
            "iters:5900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "18 + 108 = 0\n",
            "------------\n",
            "iters:6000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "120 + 106 = 0\n",
            "------------\n",
            "iters:6100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 0 0 0 1]\n",
            "109 + 84 = 0\n",
            "------------\n",
            "iters:6200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 1 0 1 1 0]\n",
            "124 + 122 = 0\n",
            "------------\n",
            "iters:6300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "43 + 70 = 0\n",
            "------------\n",
            "iters:6400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "114 + 99 = 0\n",
            "------------\n",
            "iters:6500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "21 + 116 = 0\n",
            "------------\n",
            "iters:6600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 0]\n",
            "95 + 17 = 0\n",
            "------------\n",
            "iters:6700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 0]\n",
            "78 + 32 = 0\n",
            "------------\n",
            "iters:6800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "50 + 45 = 0\n",
            "------------\n",
            "iters:6900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 0 1 0 1]\n",
            "58 + 27 = 0\n",
            "------------\n",
            "iters:7000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "88 + 20 = 0\n",
            "------------\n",
            "iters:7100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 0]\n",
            "108 + 56 = 0\n",
            "------------\n",
            "iters:7200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 0 0 0]\n",
            "118 + 42 = 0\n",
            "------------\n",
            "iters:7300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 0 1 0]\n",
            "7 + 83 = 0\n",
            "------------\n",
            "iters:7400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 1 0 1]\n",
            "84 + 121 = 0\n",
            "------------\n",
            "iters:7500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "11 + 125 = 0\n",
            "------------\n",
            "iters:7600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 0 1]\n",
            "78 + 87 = 0\n",
            "------------\n",
            "iters:7700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 1 1 1 0]\n",
            "26 + 4 = 0\n",
            "------------\n",
            "iters:7800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 1 1 0 1 0]\n",
            "126 + 92 = 0\n",
            "------------\n",
            "iters:7900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "86 + 40 = 0\n",
            "------------\n",
            "iters:8000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "61 + 109 = 0\n",
            "------------\n",
            "iters:8100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 1]\n",
            "67 + 28 = 0\n",
            "------------\n",
            "iters:8200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 0 1 0]\n",
            "109 + 61 = 0\n",
            "------------\n",
            "iters:8300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "102 + 98 = 0\n",
            "------------\n",
            "iters:8400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "8 + 123 = 0\n",
            "------------\n",
            "iters:8500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "118 + 9 = 0\n",
            "------------\n",
            "iters:8600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "44 + 69 = 0\n",
            "------------\n",
            "iters:8700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 1 1 0 1]\n",
            "126 + 63 = 0\n",
            "------------\n",
            "iters:8800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 1 1 0 0 1 1]\n",
            "119 + 124 = 0\n",
            "------------\n",
            "iters:8900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 1]\n",
            "24 + 105 = 0\n",
            "------------\n",
            "iters:9000\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "66 + 100 = 0\n",
            "------------\n",
            "iters:9100\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 0 1 0 0]\n",
            "40 + 60 = 0\n",
            "------------\n",
            "iters:9200\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "117 + 21 = 0\n",
            "------------\n",
            "iters:9300\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 0 0 1 0 0 1 1]\n",
            "12 + 7 = 0\n",
            "------------\n",
            "iters:9400\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 0 1 0 1]\n",
            "10 + 59 = 0\n",
            "------------\n",
            "iters:9500\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "62 + 49 = 0\n",
            "------------\n",
            "iters:9600\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 1 1 0 1 0]\n",
            "62 + 92 = 0\n",
            "------------\n",
            "iters:9700\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[0 1 0 0 1 0 0 1]\n",
            "25 + 48 = 0\n",
            "------------\n",
            "iters:9800\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "104 + 71 = 0\n",
            "------------\n",
            "iters:9900\n",
            "Loss:1.0\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 0 0 1 1 0]\n",
            "62 + 104 = 0\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PU67TOo00Une",
        "outputId": "0f6120c5-9118-4cd3-d2f5-69b2917c740c"
      },
      "source": [
        "import numpy as np\n",
        "from common import functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def d_tanh(x):\n",
        "    return 1/(np.cosh(x) ** 2)\n",
        "\n",
        "\n",
        "# データを用意\n",
        "# 2進数の桁数\n",
        "binary_dim = 8\n",
        "# 最大値 + 1\n",
        "largest_number = pow(2, binary_dim)\n",
        "# largest_numberまで2進数を用意\n",
        "binary = np.unpackbits(np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
        "\n",
        "input_layer_size = 2\n",
        "hidden_layer_size = 16\n",
        "output_layer_size = 1\n",
        "\n",
        "weight_init_std = 2\n",
        "learning_rate = 0.1\n",
        "\n",
        "iters_num = 10000\n",
        "plot_interval = 100\n",
        "\n",
        "# ウェイト初期化 (バイアスは簡単のため省略)\n",
        "W_in = weight_init_std * np.random.randn(input_layer_size, hidden_layer_size)\n",
        "W_out = weight_init_std * np.random.randn(hidden_layer_size, output_layer_size)\n",
        "W = weight_init_std * np.random.randn(hidden_layer_size, hidden_layer_size)\n",
        "\n",
        "# Xavier\n",
        "#W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size))\n",
        "#W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "#W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size))\n",
        "\n",
        "# He\n",
        "#W_in = np.random.randn(input_layer_size, hidden_layer_size) / (np.sqrt(input_layer_size)) * np.sqrt(2)\n",
        "#W_out = np.random.randn(hidden_layer_size, output_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "#W = np.random.randn(hidden_layer_size, hidden_layer_size) / (np.sqrt(hidden_layer_size)) * np.sqrt(2)\n",
        "\n",
        "\n",
        "\n",
        "# 勾配\n",
        "W_in_grad = np.zeros_like(W_in)\n",
        "W_out_grad = np.zeros_like(W_out)\n",
        "W_grad = np.zeros_like(W)\n",
        "\n",
        "u = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "z = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "y = np.zeros((output_layer_size, binary_dim))\n",
        "\n",
        "delta_out = np.zeros((output_layer_size, binary_dim))\n",
        "delta = np.zeros((hidden_layer_size, binary_dim + 1))\n",
        "\n",
        "all_losses = []\n",
        "\n",
        "for i in range(iters_num):\n",
        "    \n",
        "    # A, B初期化 (a + b = d)\n",
        "    a_int = np.random.randint(largest_number/2)\n",
        "    a_bin = binary[a_int] # binary encoding\n",
        "    b_int = np.random.randint(largest_number/2)\n",
        "    b_bin = binary[b_int] # binary encoding\n",
        "    \n",
        "    # 正解データ\n",
        "    d_int = a_int + b_int\n",
        "    d_bin = binary[d_int]\n",
        "    \n",
        "    # 出力バイナリ\n",
        "    out_bin = np.zeros_like(d_bin)\n",
        "    \n",
        "    # 時系列全体の誤差\n",
        "    all_loss = 0    \n",
        "    \n",
        "    # 時系列ループ\n",
        "    for t in range(binary_dim):\n",
        "        # 入力値\n",
        "        X = np.array([a_bin[ - t - 1], b_bin[ - t - 1]]).reshape(1, -1)\n",
        "        # 時刻tにおける正解データ\n",
        "        dd = np.array([d_bin[binary_dim - t - 1]])\n",
        "        \n",
        "        u[:,t+1] = np.dot(X, W_in) + np.dot(z[:,t].reshape(1, -1), W)\n",
        "        #z[:,t+1] = functions.sigmoid(u[:,t+1])\n",
        "        #z[:,t+1] = functions.relu(u[:,t+1])\n",
        "        z[:,t+1] = np.tanh(u[:,t+1]) \n",
        "        y[:,t] = functions.sigmoid(np.dot(z[:,t+1].reshape(1, -1), W_out))\n",
        "\n",
        "\n",
        "        #誤差\n",
        "        loss = functions.mean_squared_error(dd, y[:,t])\n",
        "        \n",
        "        delta_out[:,t] = functions.d_mean_squared_error(dd, y[:,t]) * functions.d_sigmoid(y[:,t])        \n",
        "        \n",
        "        all_loss += loss\n",
        "\n",
        "        out_bin[binary_dim - t - 1] = np.round(y[:,t])\n",
        "    \n",
        "    \n",
        "    for t in range(binary_dim)[::-1]:\n",
        "        X = np.array([a_bin[-t-1],b_bin[-t-1]]).reshape(1, -1)        \n",
        "\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_sigmoid(u[:,t+1])\n",
        "        #delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * functions.d_relu(u[:,t+1])\n",
        "        delta[:,t] = (np.dot(delta[:,t+1].T, W.T) + np.dot(delta_out[:,t].T, W_out.T)) * d_tanh(u[:,t+1])\n",
        "\n",
        "        # 勾配更新\n",
        "        W_out_grad += np.dot(z[:,t+1].reshape(-1,1), delta_out[:,t].reshape(-1,1))\n",
        "        W_grad += np.dot(z[:,t].reshape(-1,1), delta[:,t].reshape(1,-1))\n",
        "        W_in_grad += np.dot(X.T, delta[:,t].reshape(1,-1))\n",
        "    \n",
        "    # 勾配適用\n",
        "    W_in -= learning_rate * W_in_grad\n",
        "    W_out -= learning_rate * W_out_grad\n",
        "    W -= learning_rate * W_grad\n",
        "    \n",
        "    W_in_grad *= 0\n",
        "    W_out_grad *= 0\n",
        "    W_grad *= 0\n",
        "    \n",
        "\n",
        "    if(i % plot_interval == 0):\n",
        "        all_losses.append(all_loss)        \n",
        "        print(\"iters:\" + str(i))\n",
        "        print(\"Loss:\" + str(all_loss))\n",
        "        print(\"Pred:\" + str(out_bin))\n",
        "        print(\"True:\" + str(d_bin))\n",
        "        out_int = 0\n",
        "        for index,x in enumerate(reversed(out_bin)):\n",
        "            out_int += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out_int))\n",
        "        print(\"------------\")\n",
        "\n",
        "lists = range(0, iters_num, plot_interval)\n",
        "plt.plot(lists, all_losses, label=\"loss\")\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iters:0\n",
            "Loss:1.4942606397851363\n",
            "Pred:[1 1 1 1 0 0 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "44 + 82 = 242\n",
            "------------\n",
            "iters:100\n",
            "Loss:0.9167141286518717\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "44 + 87 = 75\n",
            "------------\n",
            "iters:200\n",
            "Loss:1.403043852437137\n",
            "Pred:[0 1 1 0 1 1 0 1]\n",
            "True:[0 0 0 1 1 0 1 0]\n",
            "15 + 11 = 109\n",
            "------------\n",
            "iters:300\n",
            "Loss:0.9137507734970177\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "2 + 96 = 130\n",
            "------------\n",
            "iters:400\n",
            "Loss:1.2920080414138737\n",
            "Pred:[0 0 1 1 0 0 0 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "66 + 13 = 49\n",
            "------------\n",
            "iters:500\n",
            "Loss:1.0733408459990186\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 1 0 1 0 1 1 1]\n",
            "1 + 86 = 1\n",
            "------------\n",
            "iters:600\n",
            "Loss:0.7781087794826216\n",
            "Pred:[0 0 0 0 0 0 1 0]\n",
            "True:[0 0 1 0 1 0 1 0]\n",
            "8 + 34 = 2\n",
            "------------\n",
            "iters:700\n",
            "Loss:1.1423451358025114\n",
            "Pred:[0 0 0 0 1 0 0 1]\n",
            "True:[0 1 1 1 1 0 1 1]\n",
            "100 + 23 = 9\n",
            "------------\n",
            "iters:800\n",
            "Loss:0.9560638042242114\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[0 1 1 0 0 0 0 0]\n",
            "19 + 77 = 220\n",
            "------------\n",
            "iters:900\n",
            "Loss:0.4873249390249614\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "43 + 28 = 103\n",
            "------------\n",
            "iters:1000\n",
            "Loss:0.6603937787598818\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 0 0 0 1 1 1]\n",
            "33 + 38 = 79\n",
            "------------\n",
            "iters:1100\n",
            "Loss:1.0248586109426197\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[1 0 1 0 1 1 1 1]\n",
            "64 + 111 = 145\n",
            "------------\n",
            "iters:1200\n",
            "Loss:0.5344998667634989\n",
            "Pred:[0 0 1 1 0 1 1 0]\n",
            "True:[0 0 1 1 0 1 1 0]\n",
            "2 + 52 = 54\n",
            "------------\n",
            "iters:1300\n",
            "Loss:0.9725984507437625\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 1 1 1 0 0 1]\n",
            "103 + 82 = 149\n",
            "------------\n",
            "iters:1400\n",
            "Loss:0.8704214551909748\n",
            "Pred:[1 0 0 0 0 1 0 1]\n",
            "True:[1 0 0 1 1 0 0 1]\n",
            "95 + 58 = 133\n",
            "------------\n",
            "iters:1500\n",
            "Loss:0.6527802794373326\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "57 + 79 = 0\n",
            "------------\n",
            "iters:1600\n",
            "Loss:1.3655452350196131\n",
            "Pred:[0 1 1 0 1 1 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "63 + 75 = 110\n",
            "------------\n",
            "iters:1700\n",
            "Loss:0.5606943536616994\n",
            "Pred:[0 1 0 0 1 1 1 1]\n",
            "True:[0 1 1 0 1 1 1 1]\n",
            "105 + 6 = 79\n",
            "------------\n",
            "iters:1800\n",
            "Loss:0.585051537245173\n",
            "Pred:[1 0 0 1 0 0 0 1]\n",
            "True:[0 1 0 1 0 0 0 1]\n",
            "72 + 9 = 145\n",
            "------------\n",
            "iters:1900\n",
            "Loss:0.43851210474291524\n",
            "Pred:[1 0 1 1 0 0 1 1]\n",
            "True:[1 0 1 1 0 1 1 1]\n",
            "78 + 105 = 179\n",
            "------------\n",
            "iters:2000\n",
            "Loss:0.6456824520812643\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 0 0 1 0 0 1 0]\n",
            "122 + 24 = 134\n",
            "------------\n",
            "iters:2100\n",
            "Loss:0.8354273448492713\n",
            "Pred:[1 0 0 0 0 0 1 0]\n",
            "True:[1 1 1 0 0 0 1 0]\n",
            "112 + 114 = 130\n",
            "------------\n",
            "iters:2200\n",
            "Loss:0.3945945065429334\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 0 1]\n",
            "69 + 68 = 139\n",
            "------------\n",
            "iters:2300\n",
            "Loss:0.5434740137333544\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 1 0 1 0 1]\n",
            "107 + 106 = 255\n",
            "------------\n",
            "iters:2400\n",
            "Loss:0.4111747899443063\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 0 0 1 0 0 0 1]\n",
            "94 + 51 = 129\n",
            "------------\n",
            "iters:2500\n",
            "Loss:0.13152710862182482\n",
            "Pred:[0 1 1 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "99 + 25 = 124\n",
            "------------\n",
            "iters:2600\n",
            "Loss:0.8412549257097124\n",
            "Pred:[1 1 1 0 1 1 0 0]\n",
            "True:[1 1 0 0 0 1 0 0]\n",
            "84 + 112 = 236\n",
            "------------\n",
            "iters:2700\n",
            "Loss:0.7835766418842738\n",
            "Pred:[0 1 1 1 1 1 1 1]\n",
            "True:[0 1 1 1 0 0 0 1]\n",
            "25 + 88 = 127\n",
            "------------\n",
            "iters:2800\n",
            "Loss:1.4212428048886263\n",
            "Pred:[1 1 0 1 0 0 1 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "51 + 89 = 210\n",
            "------------\n",
            "iters:2900\n",
            "Loss:0.8868822116273583\n",
            "Pred:[1 0 0 0 0 1 1 0]\n",
            "True:[1 1 0 0 1 0 1 0]\n",
            "88 + 114 = 134\n",
            "------------\n",
            "iters:3000\n",
            "Loss:0.6745812348203633\n",
            "Pred:[0 1 1 1 0 0 0 1]\n",
            "True:[0 1 0 1 1 0 1 1]\n",
            "35 + 56 = 113\n",
            "------------\n",
            "iters:3100\n",
            "Loss:0.5761898927831407\n",
            "Pred:[0 1 1 0 0 1 1 0]\n",
            "True:[0 0 1 0 0 0 1 0]\n",
            "29 + 5 = 102\n",
            "------------\n",
            "iters:3200\n",
            "Loss:0.623174059034553\n",
            "Pred:[0 0 0 0 1 0 1 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "23 + 71 = 10\n",
            "------------\n",
            "iters:3300\n",
            "Loss:1.0691568657242405\n",
            "Pred:[1 0 1 0 0 1 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "70 + 82 = 164\n",
            "------------\n",
            "iters:3400\n",
            "Loss:0.4950946529777651\n",
            "Pred:[0 1 1 0 1 1 1 1]\n",
            "True:[0 1 1 0 0 1 1 1]\n",
            "27 + 76 = 111\n",
            "------------\n",
            "iters:3500\n",
            "Loss:0.7979216002371726\n",
            "Pred:[0 1 1 0 0 0 1 1]\n",
            "True:[0 0 1 0 0 1 0 1]\n",
            "29 + 8 = 99\n",
            "------------\n",
            "iters:3600\n",
            "Loss:0.4757098128454094\n",
            "Pred:[0 0 1 1 1 1 1 1]\n",
            "True:[0 0 1 1 1 0 1 1]\n",
            "30 + 29 = 63\n",
            "------------\n",
            "iters:3700\n",
            "Loss:0.9270584824196867\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "66 + 115 = 187\n",
            "------------\n",
            "iters:3800\n",
            "Loss:0.5438391909355251\n",
            "Pred:[0 1 0 1 1 1 1 1]\n",
            "True:[0 1 0 1 1 0 0 1]\n",
            "42 + 47 = 95\n",
            "------------\n",
            "iters:3900\n",
            "Loss:0.4166727908481102\n",
            "Pred:[1 0 1 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 0 0 0]\n",
            "25 + 103 = 160\n",
            "------------\n",
            "iters:4000\n",
            "Loss:0.25569353175899523\n",
            "Pred:[0 1 0 0 1 1 0 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "11 + 66 = 77\n",
            "------------\n",
            "iters:4100\n",
            "Loss:0.8809627908572346\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "41 + 111 = 252\n",
            "------------\n",
            "iters:4200\n",
            "Loss:1.1013671967813592\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 1 0 1 0 0 0 0]\n",
            "93 + 115 = 252\n",
            "------------\n",
            "iters:4300\n",
            "Loss:1.3162974724949046\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[1 1 0 1 0 0 1 1]\n",
            "97 + 114 = 125\n",
            "------------\n",
            "iters:4400\n",
            "Loss:1.2330565418767518\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 0 0 0 0 1 1]\n",
            "74 + 57 = 255\n",
            "------------\n",
            "iters:4500\n",
            "Loss:0.8241392604197373\n",
            "Pred:[1 1 0 0 1 1 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "38 + 32 = 206\n",
            "------------\n",
            "iters:4600\n",
            "Loss:1.017886808689184\n",
            "Pred:[0 1 1 0 0 1 1 1]\n",
            "True:[0 1 0 0 1 1 0 1]\n",
            "18 + 59 = 103\n",
            "------------\n",
            "iters:4700\n",
            "Loss:1.2541928706600063\n",
            "Pred:[1 1 0 0 1 1 0 1]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "49 + 122 = 205\n",
            "------------\n",
            "iters:4800\n",
            "Loss:0.7313236688699082\n",
            "Pred:[0 0 1 0 0 0 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "73 + 29 = 34\n",
            "------------\n",
            "iters:4900\n",
            "Loss:0.40789898298481747\n",
            "Pred:[0 1 1 1 0 1 0 1]\n",
            "True:[0 1 1 0 0 1 0 1]\n",
            "15 + 86 = 117\n",
            "------------\n",
            "iters:5000\n",
            "Loss:0.6872960432079165\n",
            "Pred:[1 0 0 0 1 1 1 0]\n",
            "True:[1 1 0 0 1 0 1 0]\n",
            "101 + 101 = 142\n",
            "------------\n",
            "iters:5100\n",
            "Loss:1.336195522263993\n",
            "Pred:[0 1 0 0 1 0 0 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "17 + 122 = 73\n",
            "------------\n",
            "iters:5200\n",
            "Loss:1.105035916499405\n",
            "Pred:[0 0 0 1 1 0 0 0]\n",
            "True:[0 0 1 0 1 0 0 0]\n",
            "16 + 24 = 24\n",
            "------------\n",
            "iters:5300\n",
            "Loss:0.4556040898041981\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 0 1 0 1]\n",
            "59 + 58 = 125\n",
            "------------\n",
            "iters:5400\n",
            "Loss:0.4408649787752689\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "76 + 42 = 126\n",
            "------------\n",
            "iters:5500\n",
            "Loss:0.30335419549727594\n",
            "Pred:[1 0 0 0 1 0 0 0]\n",
            "True:[1 0 0 0 1 1 0 0]\n",
            "41 + 99 = 136\n",
            "------------\n",
            "iters:5600\n",
            "Loss:0.775587715078385\n",
            "Pred:[1 0 0 1 0 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "19 + 120 = 147\n",
            "------------\n",
            "iters:5700\n",
            "Loss:1.0144367392852487\n",
            "Pred:[0 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 0 0 1 1]\n",
            "33 + 82 = 125\n",
            "------------\n",
            "iters:5800\n",
            "Loss:0.7531440095222404\n",
            "Pred:[1 1 1 0 1 1 1 0]\n",
            "True:[0 1 1 0 0 1 1 0]\n",
            "33 + 69 = 238\n",
            "------------\n",
            "iters:5900\n",
            "Loss:0.4223842207976106\n",
            "Pred:[0 0 1 0 0 1 1 0]\n",
            "True:[0 0 1 1 0 0 1 0]\n",
            "27 + 23 = 38\n",
            "------------\n",
            "iters:6000\n",
            "Loss:0.6033754181172029\n",
            "Pred:[0 0 1 0 1 0 1 0]\n",
            "True:[0 0 1 0 1 1 1 0]\n",
            "11 + 35 = 42\n",
            "------------\n",
            "iters:6100\n",
            "Loss:0.8174352039038365\n",
            "Pred:[1 1 0 1 1 0 0 0]\n",
            "True:[1 0 0 0 1 0 0 0]\n",
            "52 + 84 = 216\n",
            "------------\n",
            "iters:6200\n",
            "Loss:0.29700203657821733\n",
            "Pred:[0 1 0 1 1 0 0 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "65 + 27 = 88\n",
            "------------\n",
            "iters:6300\n",
            "Loss:0.9409487152540597\n",
            "Pred:[1 1 0 1 0 1 0 0]\n",
            "True:[1 1 0 0 1 1 0 0]\n",
            "102 + 102 = 212\n",
            "------------\n",
            "iters:6400\n",
            "Loss:0.7754885523226376\n",
            "Pred:[1 0 0 0 0 0 1 1]\n",
            "True:[1 1 0 0 0 1 1 1]\n",
            "79 + 120 = 131\n",
            "------------\n",
            "iters:6500\n",
            "Loss:0.9280712275747489\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "114 + 47 = 135\n",
            "------------\n",
            "iters:6600\n",
            "Loss:0.7643403842868358\n",
            "Pred:[0 1 1 0 0 1 0 0]\n",
            "True:[0 1 1 0 1 1 0 0]\n",
            "86 + 22 = 100\n",
            "------------\n",
            "iters:6700\n",
            "Loss:0.3167620745119085\n",
            "Pred:[0 1 1 1 0 1 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "7 + 111 = 118\n",
            "------------\n",
            "iters:6800\n",
            "Loss:0.12667074701773212\n",
            "Pred:[1 0 1 1 1 0 1 1]\n",
            "True:[1 0 1 1 1 0 1 1]\n",
            "96 + 91 = 187\n",
            "------------\n",
            "iters:6900\n",
            "Loss:0.8843017427444508\n",
            "Pred:[0 1 0 1 1 0 1 0]\n",
            "True:[0 1 0 1 1 1 0 0]\n",
            "67 + 25 = 90\n",
            "------------\n",
            "iters:7000\n",
            "Loss:0.8206666963817835\n",
            "Pred:[0 0 1 0 0 0 0 0]\n",
            "True:[0 0 1 1 1 0 0 0]\n",
            "44 + 12 = 32\n",
            "------------\n",
            "iters:7100\n",
            "Loss:0.9294347032973226\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 1 0 0 1 0 0 0]\n",
            "51 + 21 = 62\n",
            "------------\n",
            "iters:7200\n",
            "Loss:0.6310499681400952\n",
            "Pred:[1 0 0 0 1 0 1 1]\n",
            "True:[1 0 0 0 1 0 1 1]\n",
            "44 + 95 = 139\n",
            "------------\n",
            "iters:7300\n",
            "Loss:1.085198787006262\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 0 0 0]\n",
            "27 + 125 = 254\n",
            "------------\n",
            "iters:7400\n",
            "Loss:0.8676745624963148\n",
            "Pred:[0 0 0 1 1 1 0 0]\n",
            "True:[0 1 1 1 1 1 0 0]\n",
            "80 + 44 = 28\n",
            "------------\n",
            "iters:7500\n",
            "Loss:0.8197655156076318\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[1 1 0 1 1 1 0 1]\n",
            "107 + 114 = 1\n",
            "------------\n",
            "iters:7600\n",
            "Loss:0.7532910481305899\n",
            "Pred:[1 1 1 1 1 1 0 0]\n",
            "True:[1 0 1 0 1 1 0 0]\n",
            "50 + 122 = 252\n",
            "------------\n",
            "iters:7700\n",
            "Loss:0.7577802744462595\n",
            "Pred:[1 0 0 0 0 0 0 1]\n",
            "True:[1 1 0 1 1 0 0 1]\n",
            "103 + 114 = 129\n",
            "------------\n",
            "iters:7800\n",
            "Loss:0.8098254001574086\n",
            "Pred:[1 1 1 1 0 1 1 1]\n",
            "True:[0 1 1 1 1 1 1 1]\n",
            "82 + 45 = 247\n",
            "------------\n",
            "iters:7900\n",
            "Loss:0.8185661250048532\n",
            "Pred:[0 0 1 1 1 1 1 0]\n",
            "True:[0 0 1 0 0 1 0 0]\n",
            "27 + 9 = 62\n",
            "------------\n",
            "iters:8000\n",
            "Loss:0.8632241254410746\n",
            "Pred:[0 1 0 1 0 0 0 1]\n",
            "True:[0 1 0 0 1 1 1 1]\n",
            "24 + 55 = 81\n",
            "------------\n",
            "iters:8100\n",
            "Loss:0.7372903367597511\n",
            "Pred:[1 0 0 0 0 1 0 0]\n",
            "True:[1 0 0 1 0 1 0 0]\n",
            "60 + 88 = 132\n",
            "------------\n",
            "iters:8200\n",
            "Loss:0.9302305671849875\n",
            "Pred:[1 1 1 1 1 0 1 1]\n",
            "True:[0 1 1 0 0 0 0 1]\n",
            "25 + 72 = 251\n",
            "------------\n",
            "iters:8300\n",
            "Loss:0.7631679875770038\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 0 1 1 0 1 0 1]\n",
            "63 + 118 = 255\n",
            "------------\n",
            "iters:8400\n",
            "Loss:0.7952968168661135\n",
            "Pred:[1 1 1 1 1 1 0 1]\n",
            "True:[0 1 1 1 0 1 1 1]\n",
            "29 + 90 = 253\n",
            "------------\n",
            "iters:8500\n",
            "Loss:0.849924059702098\n",
            "Pred:[1 1 0 1 1 1 0 0]\n",
            "True:[0 1 1 1 0 1 0 0]\n",
            "11 + 105 = 220\n",
            "------------\n",
            "iters:8600\n",
            "Loss:1.076170400918638\n",
            "Pred:[1 0 1 0 0 0 1 0]\n",
            "True:[0 1 1 1 0 1 1 0]\n",
            "42 + 76 = 162\n",
            "------------\n",
            "iters:8700\n",
            "Loss:0.972725248857823\n",
            "Pred:[0 1 0 0 0 0 0 0]\n",
            "True:[0 1 0 1 1 1 1 0]\n",
            "63 + 31 = 64\n",
            "------------\n",
            "iters:8800\n",
            "Loss:0.6541170635513865\n",
            "Pred:[0 0 1 1 0 1 1 1]\n",
            "True:[0 0 1 0 1 1 1 1]\n",
            "39 + 8 = 55\n",
            "------------\n",
            "iters:8900\n",
            "Loss:0.5919546549260165\n",
            "Pred:[0 1 0 1 0 0 1 0]\n",
            "True:[0 1 0 0 0 1 1 0]\n",
            "33 + 37 = 82\n",
            "------------\n",
            "iters:9000\n",
            "Loss:0.9690726035142381\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 0 1 0]\n",
            "80 + 42 = 254\n",
            "------------\n",
            "iters:9100\n",
            "Loss:0.8851833569943608\n",
            "Pred:[1 1 1 1 0 0 0 1]\n",
            "True:[1 0 1 0 1 0 1 1]\n",
            "80 + 91 = 241\n",
            "------------\n",
            "iters:9200\n",
            "Loss:0.8638943362619281\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 1 1 0 0 0 0]\n",
            "119 + 57 = 0\n",
            "------------\n",
            "iters:9300\n",
            "Loss:0.7373176629086314\n",
            "Pred:[1 1 1 1 1 0 0 1]\n",
            "True:[1 0 1 0 0 0 0 1]\n",
            "74 + 87 = 249\n",
            "------------\n",
            "iters:9400\n",
            "Loss:1.0707823021560965\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 0 0 0 1 0]\n",
            "85 + 13 = 254\n",
            "------------\n",
            "iters:9500\n",
            "Loss:0.7147976320478724\n",
            "Pred:[1 1 1 1 1 1 1 1]\n",
            "True:[1 1 0 1 1 1 1 1]\n",
            "119 + 104 = 255\n",
            "------------\n",
            "iters:9600\n",
            "Loss:0.697274671636662\n",
            "Pred:[1 1 1 1 0 1 0 0]\n",
            "True:[0 0 1 1 0 1 0 0]\n",
            "8 + 44 = 244\n",
            "------------\n",
            "iters:9700\n",
            "Loss:0.9197872328249963\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 1 0 0 1 0 0 0]\n",
            "78 + 122 = 0\n",
            "------------\n",
            "iters:9800\n",
            "Loss:0.8925049099668882\n",
            "Pred:[1 1 1 1 1 1 1 0]\n",
            "True:[1 0 0 0 1 0 1 0]\n",
            "98 + 40 = 254\n",
            "------------\n",
            "iters:9900\n",
            "Loss:0.7432254214104304\n",
            "Pred:[0 0 0 0 0 0 0 1]\n",
            "True:[0 0 0 0 0 0 1 1]\n",
            "0 + 3 = 1\n",
            "------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}