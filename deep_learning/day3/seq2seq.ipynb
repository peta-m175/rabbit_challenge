{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM35ZEKuKPDOtMabfJt4A6y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day3/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOu3YKqAFZzW"
      },
      "source": [
        "# Seq2Seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6FODFPtGMtp"
      },
      "source": [
        "Encoder-Decoderモデルの一種\n",
        "\n",
        "用途\n",
        "  - 機械対話や、機械翻訳など"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A53TMpw1FcKN"
      },
      "source": [
        "## Encoder RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs6OnMJhGdAx"
      },
      "source": [
        "ユーザーがインプットしたテキストデータを、単語等のトークンに区切って渡す構造\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4uJxOdvGhcZ"
      },
      "source": [
        "- Taking :文章を単語等のトークン毎に分割し、トークンごとのIDに分割する。\n",
        "- Embedding :IDから、そのトークンを表す分散表現ベクトルに変換。\n",
        "- Encoder RNN:ベクトルを順番にRNNに入力していく。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9um02dZFfbd"
      },
      "source": [
        "## Decoder RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YijAkL_GzSw"
      },
      "source": [
        "システムがアウトプットデータを、単語等のトークンごとに生成する構造"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF2N2UchFhvP"
      },
      "source": [
        "## HRED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM6LpS--HiPB"
      },
      "source": [
        "過去n-1個の発話から次の発話を生成する。\n",
        "\n",
        "Seq2seqでは、会話の文脈無視で、応答がなされたが、HREDでは、前の単語の流れに即して応答されるため、より人間らしい文章が生成される"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XNxu1aQHaxY"
      },
      "source": [
        "HREAD = Seq2Seq + Context RNN\n",
        "\n",
        "- Context RNN:\\\n",
        "  Encoder のまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する構造\\\n",
        "  ->過去の発話の履歴を加味した返答をできる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhqnGtAFFj20"
      },
      "source": [
        "## VHRED"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "182A6lk38S5e"
      },
      "source": [
        "HREDに、VAEの潜在変数の概念を追加したもの"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umApju91Fmo0"
      },
      "source": [
        "## VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdClbBxwFpSU"
      },
      "source": [
        "### オートエンコーダー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jweU9HRTIEcX"
      },
      "source": [
        "教師なし学習の一つ。\\\n",
        "そのため学習時の入力データは訓練データのみで教師データは利用しない\n",
        "\n",
        "具体例\n",
        "- MNISTの場合、28x28の数字の画像を入れて、同じ画像を出力するニューラルネットワーク"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_bLIXW3Fs5c"
      },
      "source": [
        "### VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX64y7PPIRqf"
      },
      "source": [
        "通常のオートエンコーダーの場合、何かしら潜在変数zにデータを押し込めているものの、その構造がどのような状態かわからない\\\n",
        "-> VAEはこの潜在変数zに確率分布z∼N(0,1)を仮定したもの\n",
        "\n",
        "-> データを潜在変数zの確率分布という構造に押し込めることを可能にする"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHeam0yTFwRb"
      },
      "source": [
        "## 確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W8bZGnGFywj"
      },
      "source": [
        "### P,109"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvRVeofGHG1A"
      },
      "source": [
        "- Q: 下記の選択肢から、seq2seqについて説明しているものを選べ。\n",
        "  1. 時刻に関して順方向と逆方向のRNNを構成し、それら2つの中間層表現を特徴量として利用するものである。\n",
        "  2. RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる。\n",
        "  3. 構文木などの木構造に対して、隣接単語から表現ベクトル（フレーズ）を作るという演算を再帰的に行い（重みは共通）、文全体の表現ベクトルを得るニューラルネットワークである。\n",
        "  4. RNNの一種であり、単純なRNNにおいて問題となる勾配消失問題をCECとゲートの概念を導入することで解決したものである。\n",
        "- A:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja5dGgOrHUtP"
      },
      "source": [
        "### P.119"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGIMFFH5HyVv"
      },
      "source": [
        "- Q: seq2seqとHRED、HREDとVHREDの違いを簡潔に述べよ。\n",
        "- A:\n",
        "  - seq2seq vs HRED\\\n",
        "    HREAD = Seq2Seq + Context RNN\n",
        "  - HRED vs VHRED\\\n",
        "    VHREAD = HREAD + VAEの潜在変数の概念\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khUCo6rOIo2Z"
      },
      "source": [
        "### P.128"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEtXmWbJItJv"
      },
      "source": [
        "- Q: VAEに関する下記の説明文中の空欄に当てはまる言葉を答えよ。\\\n",
        "  自己符号化器の潜在変数に____を導入したもの。\n",
        "- A:確率分布z∼N(0,1)"
      ]
    }
  ]
}