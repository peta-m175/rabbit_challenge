{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "attention_mechanism.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPj5QdfAZT2V8LkLj1xZBT7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day3/attention_mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM3MlfJjV5Ma"
      },
      "source": [
        "# Attention Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TiyymHHV6uJ"
      },
      "source": [
        "- 課題:\\\n",
        "  seq2seq の問題は長い文章への対応が難しい。\\\n",
        "  seq2seq では、2単語でも、100単語でも、固定次元ベクトルの中に入力しなければならない\n",
        "- 解決策:\\\n",
        "  文章が長くなるほどそのシーケンスの内部表現の次元も大きくなっていく、仕組みが必要\n",
        "\n",
        "-> Attention Mechanism\n",
        "  - 「入力と出力のどの単語が関連しているのか」の関連度を学習する仕組み"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQeHZdpOV70g"
      },
      "source": [
        "## 確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjGhYF-NV-No"
      },
      "source": [
        "### P.137"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSGRWng3WDjJ"
      },
      "source": [
        "- Q: RNNとword2vec、seq2seqとAttentionの違いを簡潔に述べよ\n",
        "- A:\n",
        "  - RNN vs word2vec\\\n",
        "    重みの生成ロジックが異なる。(word2vecは現実的)\\\n",
        "    RNN: ボキャブラリー数 * ボキャブラリー数\\\n",
        "    word2vec: ボキャブラリー数 * 任意の単位ベクトル\n",
        "  - seq2seq vs Attention\\\n",
        "    seq2seq: 固定長\\\n",
        "    Attention: 可変長(関連度の概念を使う)"
      ]
    }
  ]
}