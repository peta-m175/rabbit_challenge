{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7tFSIbEU1p5uzuuy5BVND",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day1/backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3zosbCMe4F1"
      },
      "source": [
        "# 誤差逆伝播法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qAjYA_se8Ec"
      },
      "source": [
        "### 数値微分"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEKL7wFT4rkU"
      },
      "source": [
        "プログラムで微小な数値を生成し擬似的に微分を計算する一般的な手法\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_m} \\approx\\frac{E(w_m+h)-E(w_m-h)}{2h}\n",
        "$$\n",
        "- デメリット\n",
        "  - 各パラメータ$w_m$それぞれについて$E(w_m+h)$や$E(w_m-h)$を計算するために、純電番の計算を繰り返し行う必要があり、負荷が大きい->誤差逆伝播法を使う"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOyaln5eTLKE"
      },
      "source": [
        "### 誤差逆伝播法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ZvZ6wiTjPE"
      },
      "source": [
        "算出された誤差を、出力層側から順に微分し、前の層前の層へと伝播。最小限の計算で各パラメータでの微分値を解析的に計算する手法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PYhf634XlcZ"
      },
      "source": [
        "- 誤差関数: 二乗誤差関数\n",
        "$$\n",
        "E(y)=\\frac{1}{2}\\sum_{j=1}^{J}(y_j-d_j)^2 = \\frac{1}{2}||y-d||^2\n",
        "$$\n",
        "- 出力層の活性化関数: 恒等写像\n",
        "$$\n",
        "y=u^{(L)}\n",
        "$$\n",
        "- 総入力の計算\n",
        "$$\n",
        "u^{(l)}=w^{(l)}z^{(l-1)}+b^{(l)}\n",
        "$$\n",
        "- 最終的に求めたいもの:\n",
        "$$\n",
        "\\frac{\\partial E}{\\partial w_{ji}^{(2)}} =\n",
        "\\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u}\\frac{\\partial u}{\\partial w_{ji}^{(2)}}\n",
        "$$\n",
        "- 各部分の計算\n",
        "$$\n",
        "\\frac{\\partial E(y)}{\\partial y} = \\frac{\\partial}{\\partial y}\\frac{1}{2}||y-d||^2 = y-d \\\\ \n",
        "\\frac{\\partial y(u)}{\\partial u}=\\frac{\\partial u}{\\partial u}=1\\\\\n",
        "\\frac{\\partial u(w)}{\\partial w_{ji}}=\\frac{\\partial}{\\partial w_{ji}}(w^{(l)}z^{(l-1)}+b^{(l)})=\\frac{\\partial}{\\partial w_{ji}}\n",
        "\\left(\n",
        "  \\left[\n",
        "    \\begin{array}{ccccc}\n",
        "      w_{11}z_1 + \\cdots + w_{1i}z_i + \\cdots + w_{1I}z_I\\\\\n",
        "      \\vdots \\\\\n",
        "      w_{j1}z_1 + \\cdots +  w_{ji}z_i + \\cdots +  w_{jI}z_I\\\\\n",
        "      \\vdots \\\\\n",
        "      w_{J1}z_1 + \\cdots +  w_{Ji}z_i + \\cdots +  w_{JI}z_I\n",
        "    \\end{array}\n",
        "  \\right] +\n",
        "    \\left[\n",
        "    \\begin{array}{c}\n",
        "      b_1\\\\\n",
        "      \\vdots \\\\\n",
        "      b_j\\\\\n",
        "      \\vdots \\\\\n",
        "      b_J\n",
        "    \\end{array}\n",
        "  \\right]\n",
        "\\right) = \n",
        "    \\left[\n",
        "    \\begin{array}{c}\n",
        "      0\\\\\n",
        "      \\vdots \\\\\n",
        "      z_i\\\\\n",
        "      \\vdots \\\\\n",
        "      0\n",
        "    \\end{array}\n",
        "  \\right] \\\\\n",
        "\\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u}\\frac{\\partial u}{\\partial w_{ji}^{(2)}} = (y-d) \\cdot\n",
        "  \\left[\n",
        "    \\begin{array}{c}\n",
        "      0\\\\\n",
        "      \\vdots \\\\\n",
        "      z_i\\\\\n",
        "      \\vdots \\\\\n",
        "      0\n",
        "    \\end{array}\n",
        "  \\right] = (y_j-d_j)z_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1K7-LHh5hJT"
      },
      "source": [
        "## 確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovcSpCT75kzp"
      },
      "source": [
        "### P.71"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cJKWiFw5tJB"
      },
      "source": [
        "- Q: 誤差逆伝播法では不要な再帰的処理を避ける事が出来る。既に行った計算結果を保持しているソースコードを抽出せよ\n",
        "- A:`delta2 = functions.d_mean_squared_error(d, y)`\n",
        "  - `delta2`を使いまわすことで計算量を減らす。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmdaSEuQWKx_"
      },
      "source": [
        "### P.76"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMnGxd4NWPfH"
      },
      "source": [
        "\n",
        "- Q: 2つの空欄に該当するソースコードを探せ\n",
        "  - $\\frac{\\partial E}{\\partial y}$: `delta2 = functions.d_mean_squared_error(d, y)`\n",
        "  1. $\\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u}$: \n",
        "  2. $\\frac{\\partial E}{\\partial y}\\frac{\\partial y}{\\partial u}\\frac{\\partial u}{\\partial w_{ji}^{(2)}}$:\n",
        "- A:\n",
        "  1. `delta2 = functions.d_mean_squared_error(d, y)`\n",
        "  2. `grad['W2'] = np.dot(z1.T, delta2)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_nApl8sz7YM"
      },
      "source": [
        "## 演習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v376IS7z9nS"
      },
      "source": [
        "https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day1/exercises/1_3_stochastic_gradient_descent.ipynb"
      ]
    }
  ]
}