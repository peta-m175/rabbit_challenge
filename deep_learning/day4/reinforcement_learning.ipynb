{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reinforcement_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZ4fjf/BHKneAy/Sb4GR3Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day4/reinforcement_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrNusF72jjdl"
      },
      "source": [
        "# 強化学習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyh6LlJd3yPj"
      },
      "source": [
        "## 強化学習とは"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LTyl-w_Zz9-"
      },
      "source": [
        "長期的に報酬を最大化できるように環境のなかで行動を選択\\\n",
        "できるエージェントを作ることを目標とする機械学習の一分野\n",
        "\n",
        "-> 行動の結果として与えられる利益(報酬)をもとに、行動を決定する原理を改善していく"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jg4ogBIbpPJ"
      },
      "source": [
        "## 強化学習の応用例"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPs7xN29bpRw"
      },
      "source": [
        "マーケティングの場合\n",
        "- 行動メール送信・非送信\n",
        "- 報酬は、 負の報酬と正の報酬の和\n",
        "  - 負の報酬: キャンペーンのコスト\n",
        "  - 正の報酬: キャンペーンで生み出される売上\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVo4-P_5JGTC"
      },
      "source": [
        "## 探索と利用のトレードオフ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRWpMAYmbF5U"
      },
      "source": [
        "環境について事前に完璧な知識があれば、最適な行動を予測し決定することは可能\\\n",
        "-> どのような顧客にキャンペーンメールを送信すると、どのような行動を行うのかが既知である状況\n",
        "\n",
        "強化学習の場合、上記仮定は成り立たないとする。\\\n",
        "不完全な知識を元に行動しながら、データを収集。最適な行動を見つけていく。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSdRyGM96T5g"
      },
      "source": [
        "トレードオフの関係性\n",
        "- 探索が足りない状態\\\n",
        "  去のデータで、ベストとされる行動のみを常に取り続ければ他にもっとベストな行動を見つけることはできない。\n",
        "- 利用が足りない状態トレードオフの関係性\n",
        "  未知の行動のみを常に取り続ければ、過去の経験が活かせない。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjBv4WCmJJ05"
      },
      "source": [
        "## 強化学習のイメージ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L3nMTVw36JA"
      },
      "source": [
        "方策$\\prod$ と 価値$V$ を学習させる\n",
        "- 方策$\\prod$:\\\n",
        "  どのように頑張るか考える\\\n",
        "  方策関数$\\prod(s,a)$\n",
        "- 価値$V$:\\\n",
        "  そういう状態が都合がいいか\\\n",
        "  行動価値関数$(Q(s,a))$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxP7xAhtJMXB"
      },
      "source": [
        "## 強化学習の差分"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYVkWcupJcDy"
      },
      "source": [
        "強化学習と通常の教師あり、教師なし学習との違い\n",
        "\n",
        "結論:目標が違う\n",
        "- 教師なし、あり学習:\\\n",
        "  データに含まれるパターンを見つけ出すおよびそのデータから予測することが目標\n",
        "- 強化学習:\\\n",
        "  優れた方策を見つけることが目標\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEowTHWdKpU1"
      },
      "source": [
        "## 強化学習の歴史"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDFFZD5OKqwX"
      },
      "source": [
        "強化学習について\n",
        "- 冬の時代があったが、計算速度の進展により大規模な状態をもつ場合の、強化学習を可能としつつある。\n",
        "- 関数近似法と、Q学習を組み合わせる手法の登場\n",
        "  - Q学習\n",
        "    - 行動価値関数を、行動する毎に更新することにより学習を進める方法\n",
        "  - 関数近似法\n",
        "    - 価値関数や方策関数を関数近似する手法のこと"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMPiWF9FJORa"
      },
      "source": [
        "## 行動価値関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuqb9lUqKJXE"
      },
      "source": [
        "- 状態価値関数:\\\n",
        "  ある状態の価値に注目する場合\n",
        "- 行動価値関数:\\\n",
        "  状態と価値を組み合わせた価値に注目する場合"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD2ron5qJPLZ"
      },
      "source": [
        "## 方策関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-0YzskXKDGl"
      },
      "source": [
        "方策ベースの強化学習手法において、ある状態でどのような行動を採るのかの確率を与える関数\n",
        "$$\n",
        "\\pi(s)=a\n",
        "$$\n",
        "関係性:\n",
        "- エージェントは方策に基づいて行動する。\n",
        "- $\\pi(s,a)$:\\\n",
        "  VやQを基にどういう行動を取るか\\\n",
        "  -> その瞬間、瞬間の行動をどうするか\n",
        "- $V^{\\pi}(s)$: 状態関数\n",
        "- $Q^{\\pi}(s,a)$: 状態 + 行動関数\n",
        "\n",
        "↑コールまで今の方策を続けたときの報酬の予測が得られる\n",
        "\n",
        "\\-\\> やり続けてどうなるか"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aupDe00YJSDx"
      },
      "source": [
        "## 方策勾配法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd7CDSlvLEUW"
      },
      "source": [
        "強化学習は\"価値関数\"と\"方策関数\"の2つが優秀ならうまく行く\\\n",
        "\\-\\> \"将来のことを考えながら\"、\"今の行動を選べる\"人\n",
        "\n",
        "\n",
        "方策反復法\\\n",
        "方策をモデル化して最適化する手法\n",
        "$$\n",
        "\\theta^{(t+1)}=\\theta^{(t)}+\\epsilon\\nabla J(\\theta)\n",
        "$$\n",
        "$J$: 方策の良さ -> 定義しなければならない\n",
        "$\\theta$:方策関数のパラメータ(NNでは誤差関数、この式では機体収益)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6JtjSp8Pd2Z"
      },
      "source": [
        "定義方法\n",
        "- 平均報酬\n",
        "- 割引報酬和\n",
        "\n",
        "行動価値関数:Q(s,a)の定義を行い、方策勾配定理が成り立つ\n",
        "$$\n",
        "\\nabla_{\\theta}J(\\theta)\n",
        "= \\nabla_{\\theta}\\sum_{0\\in A}\\pi_{\\theta}(a|s)Q^\\pi(s,a)\\\\\n",
        "=\\mathbb{E}_{\\pi\\theta}[(\\nabla_{\\theta}\\log\\pi_\\theta(a|s)Q^\\pi(s,a))]\n",
        "$$\n",
        "\n",
        "$\\pi_{\\theta}(a|s)Q^\\pi(s,a)$: ある行動をとるときの報酬\n"
      ]
    }
  ]
}