{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_rate_optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJDR27D9KxlLba8pQegqmq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day2/learning_rate_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVeyp_w54tch"
      },
      "source": [
        "# 学習率最適化手法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjTtV0hVabuS"
      },
      "source": [
        "## [勾配降下法(比較用)](https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day1/gradient_descent.ipynb#scrollTo=nqxiGbqyffZm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE1eh2Wu4xGZ"
      },
      "source": [
        "## Momentum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRpONvQAdDv5"
      },
      "source": [
        "誤差をパラメータで微分したものと学習率の積を減算した後、現在の重みに前回の重みを減算した値と慣性の積を加算する\n",
        "$$\n",
        "V_t=\\mu V_{t-1}-\\varepsilon\\nabla E \\\\\n",
        "w^{(t+1)}=w^{(t)}+V_t\n",
        "$$\n",
        "慣性: $\\mu$\n",
        "```\n",
        "self.v[key] = self.momentum* self.v[key] -self.learning_rate* grad[key]\n",
        "params[key] += self.v[key]\n",
        "```\n",
        "- メリット\n",
        "  - 局所的最適解にはならず、大域的最適解となる\n",
        "  - 谷間についてから最も低い位置(最適値)にいくまでの時間が早い"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFnF2zZ7aKpW"
      },
      "source": [
        "## AdaGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9taIJK6bG9c"
      },
      "source": [
        "誤差をパラメータで微分したものと再定義した学習率の積を減算する\n",
        "$$\n",
        "h_0=\\theta \\\\\n",
        "h_t=h_{t-1}+(\\nabla E)^2 \\\\\n",
        "w^{(t+1)}=w^{(t)}-\\varepsilon\\frac{1}{\\sqrt{h_t+\\theta}}\\nabla E\n",
        "$$\n",
        "```\n",
        "self.h[key] = np.zeros_like(val)\n",
        "self.h[key] += grad[key] * grad[key]\n",
        "params[key] -= self.learning_rate* grad[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "```\n",
        "- メリット\n",
        "  - 勾配の緩やかな斜面に対して、最適値に近づける\n",
        "- 課題\n",
        "  - 学習率が徐々に小さくなるので、鞍点問題を引き起こす事があった"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aviWozzaaOyF"
      },
      "source": [
        "## RMSProp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSbp1yf4r7yl"
      },
      "source": [
        "誤差をパラメータで微分したものと再定義した学習率の積を減算する\n",
        "$$\n",
        "h_t=\\alpha h_{t-1}+(1-\\alpha)(\\nabla E)^2 \\\\\n",
        "w^{(t+1)}=w^{(t)}-\\varepsilon\\frac{1}{\\sqrt{h_t}+\\theta}\\nabla E\n",
        "$$\n",
        "```\n",
        "self.h[key] *= self.decay_rate\n",
        "self.h[key] += (1 -self.decay_rate) * grad[key] * grad[key]\n",
        "params[key] -= self.learning_rate* grad[key] / (np.sqrt(self.h[key]) + 1e-7)\n",
        "```\n",
        "\n",
        "- メリット\n",
        "  - 局所的最適解にはならず、大域的最適解となる。\n",
        "  - ハイパーパラメータの調整が必要な場合が少ない"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H4l9d-taSL0"
      },
      "source": [
        "## Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7GsE0BLtBcT"
      },
      "source": [
        "- モメンタムの、過去の勾配の指数関数的減衰平均\n",
        "- RMSPropの、過去の勾配の2乗の指数関数的減衰平均\n",
        "\n",
        "- メリット\n",
        "  - モメンタムおよびRMSPropのメリットを孕んだアルゴリズム"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYCMRp-gaTSc"
      },
      "source": [
        "## 確認テスト"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w02yvrcdeB7E"
      },
      "source": [
        "### P.47"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLaG8DnAeGLE"
      },
      "source": [
        "- Q:モメンタム・AdaGrad・RMSPropの特徴をそれぞれ簡潔に説明せよ。\n",
        "-A:\n",
        "  - モメンタム\n",
        "    - 谷間に落ちてからの収束のスピードが早い。\n",
        "  - AdaGrad\n",
        "    - 緩やかな斜面に対して最適値に近づける。\n",
        "  - RMSProp\n",
        "    - パラメータの調整が少なくて済む。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPSOk34cOfEs"
      },
      "source": [
        "## 演習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR7DY1pnOgyz"
      },
      "source": [
        "https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/deep_learning/day2/exercises/2_4_optimizer.ipynb"
      ]
    }
  ]
}