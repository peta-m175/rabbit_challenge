{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "support_vector_machine.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMchA59dAdGJxglqqIRVoiS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/machine_learning/support_vector_machine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVa91v7OhNvV"
      },
      "source": [
        "# サポートベクターマシン(SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaX0Ijj9cc2p"
      },
      "source": [
        "## マージン"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv2PGZ2UhTX1"
      },
      "source": [
        "- 線形判別関数ともっとも近いデータ点との距離\n",
        "\n",
        "マージンが最大となる線形判別関数を求める\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmQvJJ1McFIp"
      },
      "source": [
        "## 目的関数の導出(準備)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GntEoHN_ZBRV"
      },
      "source": [
        "各クラスのデータをwの方向kへ射影した点w軸に座標\n",
        "$$\n",
        "w^{T}x+b\n",
        "$$\n",
        "線形判別関数のマージンを$\\kappa$とした時に全てのデータ点でなりたつ条件\n",
        "- 特に路肩上のデータに対しては=$\\kappa$が成り立つ\n",
        "$$\n",
        "|w^{T}x+b| \\geq \\kappa\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWIxfBnyV34L"
      },
      "source": [
        "$$\n",
        "\\rho(w,b)=\\min_{x\\in C_{t_i}=+1}\\frac{w^Tx_i}{||w||}-\\max_{x\\in C_{t_i}=-1}\\frac{w^Tx_i}{||w||}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmcMqxkQhEar"
      },
      "source": [
        "各点と決定境界との距離\n",
        "- 絶対値の定義より\n",
        "\n",
        "$$\n",
        "\\frac{|w^Tx_i+b|}{||w||}=\\frac{t_i(x^Tx_i+b)}{||w||}\n",
        "$$\n",
        "マージンは決定境界と最も距離の近い点との距離\n",
        "$$\n",
        "\\min_{i}\\frac{t_i(x^Tx_i+b)}{||w||}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tml1IMSYrGl"
      },
      "source": [
        "目的関数\n",
        "- SVMはマージンを最大化することを目標としている\n",
        "\n",
        "  マージンが最大となる直線(パラメータ)を探す\n",
        "$$\n",
        "\\max_{w,b}=[\\min_t\\frac{t_i(w^Tx_i+b)}{||w||}]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQUNPodccjFq"
      },
      "source": [
        "## SVMの主問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1W54lzhcwQf"
      },
      "source": [
        "主問題の目的関数と制約条件\n",
        "$$\n",
        "\\min_{w,b}\\frac{1}{1}||w||^2 \\\\\n",
        "t_i(w^Tx_i+b) \\geq 1 ~~~~~(i=1,2,\\dots,n)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXASOW_uc7lz"
      },
      "source": [
        "### KKT条件"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMyHv0oWc9B6"
      },
      "source": [
        "- 制約付き最適化問題において最適解が満たす条件\n",
        "\n",
        "  制約$g_i(x)\\geq0(i=1,2,\\dots,n)$のもとで、f(x)が最小となる$x^*$は以下の条件を満たす\n",
        "  - L:ラグランジュ関数\n",
        "  - $\\lambda$:ラグランジュ乗数\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_j}|x^* = 0(j=1,2,\\dots,m) \\\\\n",
        "h_i(x^*)\\leq0,\\lambda_i\\geq0,\\lambda_ig_i(x^*)=0(i-1,2,\\dots,n)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhzdRu9Pcxty"
      },
      "source": [
        "### ラグランジュの未定乗数法\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaSdOoq_eqsS"
      },
      "source": [
        "制約付き最適化問題を解くための手法\n",
        "- 定義\n",
        "\n",
        "  制約$g_i(x)\\geq 0(i=1,2,\\dots,n)$のもとで、$f(x)$が最小となる$x$は、\n",
        "  変数$\\lambda_i\\geq 0(i=1,2,\\dots,n)$を用いて新たに定義したラグランジュ関数\n",
        "\n",
        "  $L(x,\\lambda,x)=f(x)-\\sum_{i=1}^n\\lambda_ig(x)$において\n",
        "  \n",
        "  $\\frac{\\partial L}{\\partial x_j}=0(j=1,2,\\dots,m)$を満たす\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GbBxagZIo_V"
      },
      "source": [
        "ラグランジュ関数\n",
        "$$\n",
        "L(w,b,a)=\\frac{1}{2}||w||^2-\\sum_{i-1}^n(t_i(w^Tx_i+b)-1)\n",
        "$$\n",
        "ラグランジュ乗数\n",
        "$$\n",
        "a_i\\leq0(i=1,2,\\dots,n)\n",
        "$$\n",
        "最小となるw,bは以下を満たす\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w}=w-\\sum_{i=1}^na_it_ix_i=0 \\\\\n",
        "\\frac{\\partial L}{\\partial b}=-\\sum_{i=1}^na_it_i=0\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y_6R2ARdGKr"
      },
      "source": [
        "### 双対問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1R-CrJAgKKB"
      },
      "source": [
        "- 双対問題の目的関数と制約条件\n",
        "$$\n",
        "\\max_a\\sum_{i=1}^na_i-\\frac{1}{1}\\sum_{i=1}^n\\sum_{j=1}^na_ia_jt_it_jx_i^Tx_j\\\\\n",
        "\\sum_{i=1}^na_it_i=0 \\\\\n",
        "a_i \\geq0 ~~~~(i=1,2,\\dots,n)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42BrCJ6xZQ-W"
      },
      "source": [
        "#### 主問題と双対問題\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK4YAaCwZVbX"
      },
      "source": [
        "主問題の最適解と双対問題の最適解は一対一対応"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w26zohaLQomm"
      },
      "source": [
        "### 予測"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm9tUKx7Qq2P"
      },
      "source": [
        "- SVMの決定関数\n",
        "$$\n",
        "y(x)=w^Tx+b=\\sum_{i=1}^{n}a_it_ix^Tx+b\n",
        "$$\n",
        "KKT条件の相補性条件$a_i(t_i(w^Tx_i+b)-1)=0$より\n",
        "- $t_i(w^Tx_i+b)-1 >0$\n",
        "\n",
        "  予測に影響を与えない\n",
        "- $t_i(w^Tx_i+b)-1 =0$\n",
        "\n",
        "  予測に影響を与える。(サポートベクター)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Lx11A3odLbL"
      },
      "source": [
        "### サポートベクター "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wZTtrwJQgvG"
      },
      "source": [
        "分離超平面を構成する学習データはサポートベクターだけ必要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HSz5ZbzdPv8"
      },
      "source": [
        "### ソフトマージンSVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3YKDUgDNYCH"
      },
      "source": [
        "- サンプルを線形分離できないとき\n",
        "- 誤差を許容し、誤差に対してペナルティを与える\n",
        "\n",
        "マージン内に入るデータや誤分類されたデータに対して誤差を表す変数$\\xi_i$を導入する\n",
        "\n",
        "$$\n",
        "\\xi_i=1-t_i(w^Tx+b) >0\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZHZZ4pwPd3b"
      },
      "source": [
        "- 線形分離できない場合でも対応\n",
        "- パラメータCの大小で決定境界が変化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlgds5k3ZkIX"
      },
      "source": [
        "#### ソフトマージンSVMの目的関数と制約条件"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs-xnGQBP8rN"
      },
      "source": [
        "$$\n",
        "\\frac{1}{2}||w||^3+C\\sum_{i=1}^n\\xi_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89uJi0f_Zosv"
      },
      "source": [
        "### カーネルトリック"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0f8mQWUZruH"
      },
      "source": [
        "- カーネル関数(ここのみ変化):\n",
        "$$\n",
        "k(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)\n",
        "$$ \n",
        "- 高次元ベクトルの内積をスカラー関数で表現\n",
        "- 特徴空間が高次元でも計算コストを抑えられる\n",
        "目的関数\n",
        "$$\n",
        "\\max_a\\sum_{i=1}^na_i=-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^na_ia_jt_it_j\\phi(x_i)^T\\phi(x_j)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmaia6sXk9zu"
      },
      "source": [
        "## 演習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud8K3sizYgo7"
      },
      "source": [
        "https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/machine_learning/exercises/np_svm.ipynb"
      ]
    }
  ]
}