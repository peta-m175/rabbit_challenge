{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "principal_component_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMTIHb24QfEfvYzQyuPGurm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/machine_learning/principal_component_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le5Yzz4VhX-a"
      },
      "source": [
        "# 主成分分析(PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E186PXBmhbGI"
      },
      "source": [
        "学習データ\n",
        "$$\n",
        "x_i=(x_{i1},x_{i2},\\dots,x+{im}) \\in \\mathbb{R}^m\n",
        "$$\n",
        "平均(ベクトル)\n",
        "$$\n",
        "\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_i\n",
        "$$\n",
        "データ行列\n",
        "$$\n",
        "\\bar{X} = (x_1 - \\bar{x}, \\dots,x_n - \\bar{x})^{T} \\in \\mathbb{R}^{n*m}\n",
        "$$\n",
        "分散共分散行列(復習用)\n",
        "$$\n",
        "\\sum = Var(\\bar{X})=\\frac{1}{n}\\bar{X}^T\\bar{X}\n",
        "$$\n",
        "線形変換後のベクトル\n",
        "- $j$は射影軸のインデックス\n",
        "$$\n",
        "s_j=(s_{1j},\\dots,s_{nj})^T=|bar{X}a_j \\\\\n",
        "a_j \\in \\mathbb{R}^m\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MVhjpZzvEkE"
      },
      "source": [
        "係数ベクトルが変われば線形変換後の値が変化\n",
        "- 情報の量を分散の大きさと捉える\n",
        "- 線形変換後の変数の分散が最大となる射影軸を探索\n",
        "\n",
        "線形変換後の分散\n",
        "$$\n",
        "Var(s_j)=\\frac{1}{n}s_j^Ts_j=\\frac{1}{n}(\\bar{X}a_j)^T(\\bar{X}a_j)=\\frac{1}{n}a_j^T\\bar{X}\\bar{X}a_j=a_j^TVar(\\bar{X})a_j\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOaMUrclvoTE"
      },
      "source": [
        "### 制約付き最適化問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Msg11wV0vypE"
      },
      "source": [
        "- ノルムが1となる制約を入れる(制約を入れないと無限に解がある)\n",
        "\n",
        "目的関数\n",
        "$$\n",
        "arg\\ \\max_{a\\in\\mathbb{R}^m}\\ a_j^TVar(\\bar{X})a_j\n",
        "$$\n",
        "制約条件\n",
        "$$\n",
        "a_j^Ta_j=1\n",
        "$$\n",
        "\n",
        "解き方\n",
        "- ラグランジュ関数を最大にする係数ベクトルを探索(微分して0になる点)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21CHNmPxYoQk"
      },
      "source": [
        "#### ラグランジュ関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzHWNjd0s_Vn"
      },
      "source": [
        "- $\\lambda$: ラグランジュ乗数\n",
        "$$\n",
        "E(a_j)=a_j^TVar(\\bar{X})a_j-\\lambda(a_j^T-1)\n",
        "$$\n",
        "\n",
        "ラグランジュ関数を微分して最適解を求める\n",
        "\n",
        "微分\n",
        "$$\n",
        "\\frac{\\partial E(a_j)}{\\partial a_j}=2Var(\\bar{X})a_j -2 \\lambda a_j = 0\n",
        "$$\n",
        "解\n",
        "$$\n",
        "Var(\\bar{X}a_j)=\\lambda a_j\n",
        "$$\n",
        "-> 固有値・固有ベクトルの形になる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9mx8-JayP-T"
      },
      "source": [
        "### 寄与率"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gREw75ckdGHG"
      },
      "source": [
        "- 第k主成分の分散は主成分に対応する固有値\n",
        "$$\n",
        "V_{total} = \\sum_{i-1}^m\\lambda_i\n",
        "$$\n",
        "- 寄与率：第k主成分の分散の全分散に対する割合(第k主成分が持つ情報量の割合)\n",
        "$$\n",
        "c_k = \\frac{\\lambda_k}{\\sum_{i=1}^m\\lambda_i}\n",
        "$$\n",
        "- 累積寄与率：第1-k主成分まで圧縮した際の情報損失量の割合\n",
        "$$\n",
        "r_k = \\frac{\\sum_{i=1}^m\\lambda_j}{\\sum_{i=1}^m\\lambda_i}\n",
        "$$\n",
        "\n",
        "AutoEncoderでの次元圧縮でも利用される。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnpoaznUxRcB"
      },
      "source": [
        "## 演習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCQebl07kiit"
      },
      "source": [
        "https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/machine_learning/exercises/np_pca.ipynb"
      ]
    }
  ]
}