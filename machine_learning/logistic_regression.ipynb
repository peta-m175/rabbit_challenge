{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "logistic_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBJyTQ0w5dak9KqJZr7Tp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/machine_learning/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pod3P1RxAEgx"
      },
      "source": [
        "# ロジスティック回帰モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLfQtUCyAUHS"
      },
      "source": [
        "## 分類問題(クラス分類)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBt1-2BBD4i"
      },
      "source": [
        "ある入力(数値)からクラスに分類する問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEbTb1rNA7-r"
      },
      "source": [
        "\n",
        "### 分類で扱うデータ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dSVV4nuBHga"
      },
      "source": [
        "- 入力(各要素を説明変数または特徴量と呼ぶ)\n",
        "  - m次元のベクトル(m=1の場合はスカラ)\n",
        "- 出力(目的変数)\n",
        "  - 0 or 1の値\n",
        "- タイタニックデータ、IRISデータなど\n",
        "\n",
        "説明変数\n",
        "$$\n",
        "x=(x_1,x_2,\\dots,x+m)^{T} \\in \\mathbb{R}^{m}\n",
        "$$\n",
        "目的変数\n",
        "$$\n",
        "y \\in \\{0,1\\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ACpgzlIBNE6"
      },
      "source": [
        "## ロジスティック線形回帰モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reA75OO7BQQi"
      },
      "source": [
        "分類問題を解くための教師あり機械学習モデル(教師データから学習)\n",
        "- 入力とm次元パラメータの線形結合をシグモイド関数に入力\n",
        "- 出力はy=1になる確率の値になる\n",
        "\n",
        "パラメータ\n",
        "$$\n",
        "w=(w_1,w_2,\\dots,w_m)^{T} \\in \\mathbb{R}^{m}\n",
        "$$\n",
        "線形結合\n",
        "$$\n",
        "\\hat{y}=w^{T}x + w_0 =\\sum_{j=1}^{m}w_{j}x_{j} + w_0\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow2Pk2wzBZkK"
      },
      "source": [
        "### シグモイド関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4iZ-_FMB8Lc"
      },
      "source": [
        "- 入力は実数・出力は必ず0~1の値\n",
        "- 単調増加関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnXu3wxHCGxk"
      },
      "source": [
        "#### パラメータが変わるとシグモイド関数の形が変わる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLMf2X_yCLX8"
      },
      "source": [
        "- aを増加させると，x=0付近での曲線の勾配が増加\n",
        "- aを極めて大きくすると，単位ステップ関数(x<0でf(x)=0，x>0でf(x)=1となるような関数)に近づきます\n",
        "- バイアス変化は段差の位置a=0.2a=0.5a=1a= 10ロジスティック回帰モデル\n",
        "\n",
        "$$\n",
        "\\sigma = \\frac{1}{1+\\exp(-ax)} \n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y_6N6MOC15-"
      },
      "source": [
        "#### シグモイド関数の性質"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igugxg3uDUpu"
      },
      "source": [
        "- シグモイド関数の微分は、シグモイド関数自身で表現することが可能\n",
        "- 尤度関数の微分を行う際にこの事実を利用すると計算が容易\n",
        "\n",
        "$$\n",
        "\\frac{\\partial\\sigma(x)}{\\partial x} = \\frac{\\partial}{\\partial\\sigma} (\\frac{1}{1+\\exp(-ax)}) \\\\\n",
        "% 連鎖律\n",
        "= (-1)\\cdot \\{1+\\exp(-ax)\\}^{-2} \\cdot \\exp(-ax) \\cdot(-a)\\\\\n",
        "= \\{\\frac{a \\exp(-ax)}{\\{1+\\exp(-ax)\\}^2}\\} = \\frac{a}{1+\\exp(-ax)} \\cdot \\frac{1+\\exp(-ax) -1}{1+\\exp(-ax)} \\\\\n",
        "= a\\sigma(x)(1-\\sigma(x))\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq_-0kqaDVBW"
      },
      "source": [
        "##### シグモイド関数の出力をY=1になる確率に対応させる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tep3XQVUDVnp"
      },
      "source": [
        "- データの線形結合を計算\n",
        "- シグモイド関数に入力▶出力が確率に対応\n",
        "- [表記] i番目データを与えた時のシグモイド関数の出力をi番目のデータがY=1になる確率とする\n",
        "\n",
        "求めたい値\n",
        "$$\n",
        "P(Y=1|x) = \\sigma(w_0 + w_1 x_1 + \\dots + w_m x_m)\n",
        "$$\n",
        "- $P(Y=1|x)$: 説明変数の実現値が与えられた際にY=1になる確率\n",
        "- $w_0 + w_1 x_1 + \\dots + w_m x_m$: データのパラメーターに対する線形結合\n",
        "\n",
        "表記\n",
        "$$\n",
        "p_i = \\sigma(w_0 + w_1 x_{i1} + \\dots + w_m x_{im})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIay3DaTDVqv"
      },
      "source": [
        "## 最尤推定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT0eZbyyLzjJ"
      },
      "source": [
        "世の中には様々な確率分布が存在する\n",
        "- 正規分布、t分布、ガンマ分布、一様分布、ディリクレ分布・・・\n",
        "- ロジスティック回帰モデルではベルヌーイ分布を利用する\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suPYlJskL_uh"
      },
      "source": [
        "### ベルヌーイ分布"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6slVpugMCeR"
      },
      "source": [
        "- 数学において、確率pで1、確率1 − p で0をとる、離散確率分布(例：コインを投げ)\n",
        "- 「生成されるデータ」は分布のパラメータによって異なる(この場合は確率p)\n",
        "\n",
        "ベルヌーイ分布に従う確率変数Y\n",
        "$$\n",
        "Y〜Be(p) \\\\\n",
        "P(y) = p^y (1-p)^{1-y}\n",
        "$$\n",
        "Y=0, Y=1になる確率をまとめて表現"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3biFo0lNmg-"
      },
      "source": [
        "### 同時確率"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jFjBaomNsI-"
      },
      "source": [
        "- あるデータが得られた時、それが同時に得られる確率\n",
        "- 確率変数は独立であることを仮定すると、それぞれの確率の掛け算となる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNkx7Hp4Nwl9"
      },
      "source": [
        "### 尤度関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl4k7EY5N4ym"
      },
      "source": [
        "- データは固定し、パラメータを変化させる\n",
        "- 尤度関数を最大化するようなパラメータを選ぶ推定方法を最尤推定という\n",
        "\n",
        "1回の試行でy=y_1になる確率\n",
        "$$\n",
        "P(y)=p^{y}(1 - p)^{1-y}\n",
        "$$\n",
        "n回の試行でy_1~y_nが同時に起こる確率(p固定)\n",
        "\n",
        "また\n",
        "y1~y_nのデータが得られた際の尤度関数\n",
        "$$\n",
        "P(y_i,y_2,\\dots,y_n ;p) = \\prod_{i=1}^{n}p^{y_i}(1-p)^{1-y_i}\n",
        "$$\n",
        "- n回の試行でy_1~y_nが同時に起こる確率(p固定)\n",
        "  - 既知: $p$\n",
        "- y1~y_nのデータが得られた際の尤度関数\n",
        "  - 既知: $y_i,y_2,\\dots,y_n$\n",
        "  - 未知: $p$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nb66pIucN84u"
      },
      "source": [
        "### ロジスティック回帰モデルの最尤推定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKeGeaCcDVyP"
      },
      "source": [
        "- 確率pはシグモイド関数となるため、推定するパラメータは重みパラメータとなる\n",
        "- $(x_1,y_1),(x_2,y_2),\\dots,(x_n,y_n)$を生成するに至った尤もらしいパラメータを探す\n",
        "\n",
        "$$\n",
        "P(Y=y_1|x_1) = p_{1}^{y_1}(1-p_1)^{1-y_1} = \\sigma(w^T x_1)^{y_1}(1- \\sigma(w^T x _1))^{1-y_1} \\\\\n",
        "P(Y=y_2|x_2) = p_{2}^{y_2}(1-p_2)^{1-y_2} = \\sigma(w^T x_2)^{y_2}(1- \\sigma(w^T x _2))^{1-y_2} \\\\\n",
        "\\dots \\\\\n",
        "P(Y=y_n|x_n) = p_{n}^{y_n}(1-p_n)^{1-y_n} = \\sigma(w^T x_n)^{y_n}(1- \\sigma(w^T x _n))^{1-y_n}\n",
        "$$\n",
        "未知: $w^T$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7CBvR3fem5-"
      },
      "source": [
        "y_1~y_nのデータが得られた際の尤度関数\n",
        "$$\n",
        "% 確率変数が独立を仮定▶確率の積に分解可能\n",
        "P(y_1,y_2,\\dots,y_n|w_0,w_1,\\dots,w_m) = \\prod_{i=1}^{n}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n",
        "= \\prod_{i=1}^{n}\\sigma(w^Tx_i)^{y_i}(1-\\sigma(w^Tx_i))^{1-y_i} \\\\\n",
        "% 尤度関数はパラメータのみに依存する関数\n",
        "= L(w)\n",
        "$$\n",
        "尤度関数Eを最大とするパラメータを探索"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxteG2e2gBkn"
      },
      "source": [
        "##### 尤度関数を最大とするパラメータを探す(推定)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1IwG_z0gDPe"
      },
      "source": [
        "- 対数をとると微分の計算が簡単\n",
        "  - 同時確率の積が和に変換可能\n",
        "  - 指数が積の演算に変換可能\n",
        "- 対数尤度関数が最大になる点と尤度関数が最大になる点は同じ\n",
        "  - 対数関数は単調増加(ある尤度の値がx1 < x2の時、必ずlog(x1) < log(x2)となる)\n",
        "- 「尤度関数にマイナスをかけたものを最小化」し、「最小2乗法の最小化」と合わせる\n",
        "\n",
        "$$\n",
        "E(w_0,w_1,\\dots,w_m)=-\\log L(w_0,w_1,\\dots,w_m) \\\\\n",
        "= -\\sum_{i=1}^{n}\\{y_i\\log p_i+(1-y_i)\\log(1-p_i)\\}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38H-WKpzSM0u"
      },
      "source": [
        "### 勾配降下法(Gradient descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoXRgHHiXhnj"
      },
      "source": [
        "- 反復学習によりパラメータを逐次的に更新するアプローチの一つ\n",
        "- $\\eta$は学習率と呼ばれるハイパーパラメータでモデルのパラメータの収束しやすさを調整\n",
        "\n",
        "なぜ必要か？\n",
        "- [線形回帰モデル(最小2乗法)]  ▶MSEのパラメータに関する微分が0になる値を解析に求めることが可能\n",
        "- [ロジスティック回帰モデル(最尤法)] ▶対数尤度関数をパラメータで微分して0になる値を求める必要があるのだが、解析的にこの値を求めることは困難である。\n",
        "\n",
        "$$\n",
        "w(k+1)=w^{k}-\\eta \\frac{\\partial E(w)}{\\partial w}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kQtjhxGYNSe"
      },
      "source": [
        "対数尤度関数を係数とバイアスに関して微分\n",
        "$$\n",
        "% 連鎖律\n",
        "\\frac{\\partial E(w)}{\\partial w}  = \\sum_{i=1}^{n}\\frac{\\partial E_i(w)}{\\partial p_i} \\frac{\\partial p_i}{\\partial w} \\\\\n",
        "% 対数尤度関数のpに関する微分\n",
        "= \\sum_{i=1}^{n}(\\frac{y_i}{p_i}-\\frac{1-y_i}{1-p_i})\\frac{\\partial p_i}{\\partial w} \\\\\n",
        "% シグモイド関数の微分\n",
        "= \\sum_{i=1}^{n}(\\frac{y_i}{p_i}-\\frac{1-y_i}{1-p_i})p_i(1-p_i)x_i\n",
        "% 式を整理\n",
        "= -\\sum_{i=1}^{n}(y_i(1-p_i) - p_i(1-y_i))x_i \\\\\n",
        "= -\\sum_{i=1}^{n}(y_i - p_i)x_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnlYbdyhYOHN"
      },
      "source": [
        "パラメータが更新されなくなった場合、それは勾配が0になったということ。\n",
        "少なくとも反復学習で探索した範囲では最適な解がもとめられたことになる。\n",
        "$$\n",
        "w^(k+1)=w^k + \\eta \\sum_{i=1}^{n}(y_i-p_i)x_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqG5bN3AXzkW"
      },
      "source": [
        "勾配降下法では、パラメータを更新するのにN個全てのデータに対する和を求める必要がある。\n",
        "- nが巨大になった時にデータをオンメモリに載せる容量が足りない、計算時間が莫大になるなどの問題がある\n",
        "- 確率的勾配降下法を利用して解決"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5oAJ5Q2SIGf"
      },
      "source": [
        "### 確率的勾配降下法(SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23v2WJQsXPTC"
      },
      "source": [
        "- データを一つずつランダムに(「確率的」に)選んでパラメータを更新\n",
        "- 勾配降下法でパラメータを1回更新するのと同じ計算量でパラメータをn回更新できるので効率よく最適な解を探索可能\n",
        "\n",
        "$$\n",
        "w(k+1)=w^{k}+\\eta(y_i-p_i)x_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTvaLAx0DV03"
      },
      "source": [
        "## 分類の評価方法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZH2zZQQVZVb"
      },
      "source": [
        "||結果(Positive)|結果(Positive)|\n",
        "|:---|:---:|:---:|\n",
        "|予測(Positive)|<font color=\"blue\">真陽性(True Positive)<br>~正しくpositiveと判別した個数</font>|<font color=\"red\">偽陰性(False Positive)<br>~間違えてpositiveと判別した個数</font>|\n",
        "|予測(Positive)|<font color=\"red\">偽陽性(False Negative)<br>~間違えてNegativeと判別した個数</font>|<font color=\"blue\">真陰性(True Negative)<br>~正しくNegativeと判別した個数</font>|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IM8-0CopRE6B"
      },
      "source": [
        "- 正解した数/ 予測対象となった全データ数\n",
        "- どのような問題があるか？\n",
        "  - 分類したいクラスにはそれぞれ偏りがあることが多い\n",
        "  - この場合、単純な正解率はあまり意味をなさないことがほとんど\n",
        "\n",
        "$$\n",
        "Accuracy = \\frac{TP+TN}{TP+FN+FP+TN}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXkaLsWRe21"
      },
      "source": [
        "- False Positive\n",
        "  - 正常 -> 異常と判定\n",
        "  - 確認の工数がかかかる。\n",
        "- False Negative\n",
        "  - 異常 -> 正常と判定\n",
        "  - 本当に検知したい脅威が見えなくなる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUMPMdjMDiZH"
      },
      "source": [
        "### 再現率(Recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUHV9hFkDtwG"
      },
      "source": [
        "- 「本当にPositiveなもの」の中からPositiveと予測できる割合(NegativeなものをPositiveとしてしまう事象については考えていない)\n",
        "- 「誤り(False Positive)が多少多くても抜け漏れは少ない」予測をしたい際に利用\n",
        "$$\n",
        "Recall = \\frac{TP}{TP+FN}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQAwM1kDDlZO"
      },
      "source": [
        "### 適合率(Precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QujDXakDojW"
      },
      "source": [
        "- モデルが「Positiveと予測」したものの中で本当にPositiveである割合(本当にPositiveなものをNegativeとしてしまう子については考えていない)\n",
        "- 見逃し(False Negative)が多くてもより正確な」予測をしたい際に利用\n",
        "$$\n",
        "Precision = \\frac{TP}{TP+FP}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5ovabGUDpom"
      },
      "source": [
        "### F値(F-measure)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-P2-0eOWixY"
      },
      "source": [
        "- 理想的にはどちらも高いモデルがいいモデルだが、両者はトレードオフの関係にあり、どちらかを小さくすると、もう片方の値が大きくなってしまう。\n",
        "- PrecisionとRecallの調和平均\n",
        "  - RecallとPrecisionのバランスを示している。高ければ高いほどRecallとPrecisionがともに高くなる\n",
        "$$\n",
        "\\frac{2Recall\\cdot Precision}{Recall+Precision}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgTTSsP2kGDY"
      },
      "source": [
        "## 演習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4xZvFZqkIHf"
      },
      "source": [
        "https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/machine_learning/exercises/np_logistic_regression.ipynb"
      ]
    }
  ]
}