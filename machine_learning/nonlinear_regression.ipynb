{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nonlinear_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2uKUiZozK/YFai4rgBOEd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/machine_learning/nonlinear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cisAguaBtxc9"
      },
      "source": [
        "# 非線形回帰モデル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XpBdwgpt1Cj"
      },
      "source": [
        "- 複雑な非線形構造を内在する現象に対して、非線形回帰モデリングを実施\n",
        "  - データの構造を線形で捉えられる場合は限られる\n",
        "  - 非線形な構造を捉えられる仕組みが必要"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wi8lhqhuxRO"
      },
      "source": [
        "## 基底展開法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N38kgvZCvDTn"
      },
      "source": [
        "- 回帰関数として、基底関数と呼ばれる既知の非線形関数とパラメータベクトルの線型結合を使用\n",
        "- 未知パラメータは線形回帰モデルと同様に最小2乗法や最尤法により推定\n",
        "\n",
        "$$\n",
        "y_i=f(x_i)+\\varepsilon_i \\\\\n",
        "y_i = w_0 + \\sum_{j=1}^{m} w_j \\phi_{j}(x_i)+\\varepsilon_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suQ76J3cuIFm"
      },
      "source": [
        "### よく使われる基底関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyLHtfdJvHw3"
      },
      "source": [
        "- 多項式関数\n",
        "- ガウス型基底関数\n",
        "- スプライン関数/ Bスプライン関数"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTYUF5MvuqU-"
      },
      "source": [
        "### 1次元の基底関数に基づく非線形回帰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBljkACSvOzZ"
      },
      "source": [
        "- 多項式(1~9次)\n",
        "$$\n",
        "\\phi_j=x^j\n",
        "$$\n",
        "- ガウス型基底\n",
        "$$\n",
        "\\phi_j(x)=\\exp\\{\\frac{(x-\\mu_j)^2}{2h_j}\\}\n",
        "$$\n",
        "  - 正規分布の形になっている。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu8Vb_LSvMvP"
      },
      "source": [
        "### 2次元の基底関数に基づく非線形回帰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbe6UYjDv-Dp"
      },
      "source": [
        "- 2地毛っbガウス型基底関数\n",
        "$$\n",
        "\\phi_j(x)=\\exp\\{\\frac{(x-\\mu_j)^T(x-\\mu_j)}{2h_j}\\}\n",
        "$$\n",
        "  - $2h_j$がバンド幅"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiSUqNVrwXZj"
      },
      "source": [
        "説明変数\n",
        "$$\n",
        "x_i=(x_{i1},x_{i2},\\dots,x_{im}) \\in \\mathbb{R}^{m}\n",
        "$$\n",
        "非変性関数ベクトル\n",
        "$$\n",
        "\\phi(x_i)=(\\phi_1(x_i),\\phi_2(x_i),\\dots,\\phi_k(x_i))^{T} \\in \\mathbb{R}^{k}\n",
        "$$\n",
        "非線形関数の計画行列\n",
        "$$\n",
        "\\phi^{(train)}=(\\phi(x_1),\\phi(x_2),\\dots,\\phi(x_n))^{T} \\in \\mathbb{R}^{n*k}\n",
        "$$\n",
        "最尤法による予測値\n",
        "$$\n",
        "\\hat{y}=\\phi(\\phi^{(train)T}\\phi^{(train)})^{-1}\\phi^{(train)T}y^{(train)}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj_ZxA04y0xT"
      },
      "source": [
        "## 未学習(underfitting)と過学習(overfitting)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z9tvf3gy59q"
      },
      "source": [
        "- 学習データに対して、十分小さな誤差が得られないモデル→未学習\n",
        "  - (対策)モデルの表現力が低いため、表現力の高いモデルを利用する\n",
        "- 小さな誤差は得られたけど、テスト集合誤差との差が大きいモデル→過学習\n",
        "  - (対策1) 学習データの数を増やす\n",
        "  - (対策2) 不要な基底関数(変数)を削除して表現力を抑止\n",
        "  - (対策3) 正則化法を利用して表現力を抑止"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU7UWw6R8o-i"
      },
      "source": [
        "## 正則化法(罰則化法)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCrpNT9R4tz2"
      },
      "source": [
        "モデルの複雑さを調整する2つの方法\n",
        "\n",
        "- 不要な基底関数を削除\n",
        "  - 基底関数の数、位置やバンド幅によりモデルの複雑さが変化\n",
        "  - 解きたい問題に対して多くの基底関数を用意してしまうと過学習の問題がおこるため、適切な基底関数を用意(CVなどで選択)\n",
        "- 正則化法(罰則化法)\n",
        "  - 「モデルの複雑さに伴って、その値が大きくなる<font color=\"blue\">正則化項(罰則項)</font>を課した関数」を最小化\n",
        "  - 正則化項(罰則項)\n",
        "    - 形状によっていくつもの種類があり、それぞれ推定量の性質が異なる(次スライド)\n",
        "  - 正則化(平滑化)パラメータ\n",
        "    - モデルの曲線のなめらかさを調節▶適切に決める必要あり\n",
        "\n",
        "$$\n",
        "S_{\\gamma} = (y - \\phi^{n * k}w)^{T}(y-\\phi w) + \\gamma R(w) \\\\\n",
        "\\gamma(>0)\n",
        "$$\n",
        "$\\phi$: 基底関数の数(k)が増加するとパラメータが増加し、残差は減少(モデルが複雑化)\n",
        "\n",
        "$\\gamma$: 制約面の大きさ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzr6-gCc7HgQ"
      },
      "source": [
        "- 正則化項(罰則項)の役割\n",
        "  - 無い▶最小2乗推定量\n",
        "  - L2ノルムを利用▶<font color=\"blue\">Ridge推定量</font>\n",
        "  - L1ノルムを利用▶<font color=\"red\">Lasso推定量</font>\n",
        "\n",
        "- 正則化パラータの役割\n",
        "  - 小さく▶制約面が大きく\n",
        "  - 大きく▶制約面が小さく"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUhaqko65Cf0"
      },
      "source": [
        "## 汎化性能"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRNBCRmmwcOK"
      },
      "source": [
        "- 学習に使用した入力だけでなく、これまで見たことのない新たな入力に対する予測性能\n",
        "  - (学習誤差ではなく)汎化誤差(テスト誤差)が小さいものが良い性能を持ったモデル。\n",
        "  - 汎化誤差は通常、学習データとは別に収集された検証データでの性能を測ることで推定\n",
        "- バイアス・バリアンス分解\n",
        "  - ref: https://www.hellocybernetics.tech/entry/2017/01/24/100415\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPyL_A4H9J-V"
      },
      "source": [
        "## ホールドアウト法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nVgx4y-96hk"
      },
      "source": [
        "- 有限のデータを学習用とテスト用の2つに分割し、「予測精度」や「誤り率」を推定する為に使用\n",
        "　　- 学習用を多くすればテスト用が減り学習精度は良くなるが、性能評価の精度は悪くなる\n",
        "  - 逆にテスト用を多くすれば学習用が減少するので、学習そのものの精度が悪くなることになる。\n",
        "  - <font color=\"red\">手元にデータが大量にある場合を除いて、良い性能評価を与えないという欠点</font>がある。\n",
        "- 基底展開法に基づく非線形回帰モデルでは、基底関数の数、位置、バンド幅の値とチューニングパラメータをホールドアウト値を小さくするモデルで決定する非線形回帰モデル\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdStvFoB-ise"
      },
      "source": [
        "## クロスバリデーション(交差検証)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiiTguQ3_Are"
      },
      "source": [
        "ホールドアウト法をnパターンに分割して、それぞれ精度を測定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTy-Dag4_BLG"
      },
      "source": [
        "## グリッドサーチ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXkT_jl9_EX_"
      },
      "source": [
        "- 全てのチューニングパラメータの組み合わせで評価値を算出\n",
        "- 最も良い評価値を持つチューニングパラメータを持つ組み合わせを、「いいモデルのパラメータ」として採用\n",
        "\n",
        "$(\\lambda,m)$座標として\n",
        "$$\n",
        "m = (m_1,m_2,\\dots,m_c)^T \\\\\n",
        "\\lambda =(\\lambda_1,\\lambda_2,\\dots,\\lambda_c)^T\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTTkhIud_ujA"
      },
      "source": [
        "## 演習"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tqyXqICi95N"
      },
      "source": [
        "https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/machine_learning/exercises/skl_nonlinear_regression.ipynb"
      ]
    }
  ]
}