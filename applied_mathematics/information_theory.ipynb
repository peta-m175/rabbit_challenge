{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "information_theory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiAokf+DSjMtZf5N2Vze3T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peta-m175/rabbit_challenge/blob/master/applied_mathematics/information_theory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQb3aSstsifg"
      },
      "source": [
        "# 情報理論"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U9Ucew1sqPv"
      },
      "source": [
        "## 自己情報量"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV8CP_U7eDdK"
      },
      "source": [
        "- 対数の底が2のとき，単位はビット(bit)\n",
        "- 対数の底がネイピアのeのとき，単位は(nat)\n",
        "\n",
        "$$\n",
        "I(x)=-\\log{(P(x))} = \\log{(W(x))}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgi4mNq-suI2"
      },
      "source": [
        "## シャノンエントロピー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ulN4kgTetLV"
      },
      "source": [
        "- 微分エントロピーともいう（誤訳？）\n",
        "- 自己情報量の期待値\n",
        "$$\n",
        "H(x)= E(I(x)) \\\\\n",
        "= -E(\\log{(P(x))}) \\\\\n",
        "= -\\sum(P(x)\\log{(P(x))})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUCzpc6JsyA-"
      },
      "source": [
        "## カルバック・ライブラーダイバージェンス"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6KP29ghszi2"
      },
      "source": [
        "- 同じ事象・確率変数における異なる確率分布P,Qの違いを表す\n",
        "$$\n",
        "D_{KL}(P||Q) = \\mathbb{E}_{x〜P} [\\log\\frac{P(x)}{Q(x)}] = \\mathbb{E}_{x 〜P} [\\log{P(x)}-{Q(x)}]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st1g4MRIs1kn"
      },
      "source": [
        "## 交差エントロピー"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBoJwRGh17DW"
      },
      "source": [
        "- KLダイバージェンスの一部分を取り出したもの\n",
        "- Qについての自己情報量をPの分布で平均している\n",
        "\n",
        "$$\n",
        "H(P,Q) = H(P) + D_{KL}(P||Q) \\\\\n",
        "H(P,Q) = - \\mathbb{E}_{x〜P} \\log Q(x)\n",
        "$$"
      ]
    }
  ]
}